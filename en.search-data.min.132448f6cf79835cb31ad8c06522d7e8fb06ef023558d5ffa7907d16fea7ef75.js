'use strict';(function(){const b={};b.doc={id:'id',field:['title','content'],store:['title','href','section']};const a=FlexSearch.create('balance',b);window.bookSearchIndex=a,a.add({id:0,href:'/lxd-compose-docs/docs/concepts/',title:"Concepts",section:"LXD Compose",content:"Concepts #  Inside the lxd-compose world you can create different environments (or infrastructures) that are described by different YAML file read by lxd-compose on the directories defined on the parameter env_dirs of the configuration file.\nAn environment contains one or more project. A project describe a target service or a subset of the services supplied from an infrastructure. For example, a project could contains all services in the External DMZ like balancer, proxy, etc.\nA project contains one or more groups. A group describes the list of nodes that supply a service. For example, a group could contain all nodes supply Nginx reverse balancer. A group communicate with a single LXD instance or Cluster defined on connection attribute.\nA groups contains one or more nodes. A node is an LXD container (or in the near future also VM because LXD now supports QEMU integration).\nA node is based on an image (available from official Canonical service or any LXD Server supply images or yet from a Simplestreams server build with simplestreams-builder tool.\nA node is created with a list of LXD profiles that are used to configure networking, CPU limit, memory limit, host path mounted inside the container, etc.\nIn the deploy workflow lxd-compose generates optionally files to inject in the container through jinja2 engine (it uses j2cli tool) or directly through Golang\u0026rsquo;s template engine based on the MottainaiCI code. These files are then pushed inside the container.\nThe steps to configure the container are called hooks and they are characterized by shell commands execute inside the container or in the host where is run lxd-compose and where the environment is initialized with all the variables of the project. These hooks could be executed in different phases of the deployment and based on the choice of the flags.\nAt the moment, the execution of hooks and the creation of the containers is sequential but in the next releases will be integrate a Directed Acyclic Graph (DAG) management of the hooks/tasks for parallel deploy or complex scenarios.  "}),a.add({id:1,href:'/lxd-compose-docs/docs/tutorial/',title:"Tutorial",section:"LXD Compose",content:"Tutorial #  Hereinafter, how to create your first project in five steps. Enjoy!\nSetup LXD Environment #  Step 1: Install LXD #  Ubuntu Linux $\u0026gt; apt-get install lxd-installer $\u0026gt; apt-get install zfsutils-linux $\u0026gt; apt-get install curl In Ubuntu LXD is not available as a single package, but it uses snapd, so lxd-installer is the installer package of LXD. The installation of zfsutils is not mandatory but it prepares the environment if you want to use ZFS with LXD.\nUbuntu supplies LXD through snapd service. There isn\u0026rsquo;t a real package.\nMacaroni OS This is for Macaroni Funtoo release:\n$\u0026gt; luet install -y app-emulation/lxd If you follow the steps described in the step 1b lxd-compose manages the installation itself.\nFuntoo Linux Install and compile LXD with:\n$\u0026gt; emerge app-emulation/lxd The Funtoo documentaion is available here.\nIf you follow the steps described in the step 1b lxd-compose manages the installation itself.\n Follow the installation steps to install lxd-compose binary.\nEnable LXD API in binding (this is not needed if you want to use local remote through unix socket):\nStep 1a: Setup LXD instance manually #  Some initial steps could be done manually for a simple installation. So, just after that the service is running proceed with the configuration of the API port binding and password with:\n$\u0026gt; lxc config set core.https_address [::]:8443 $\u0026gt; lxc config set core.trust_password mypassword If it\u0026rsquo;s used LXD only with unix socket the previous commands are not needed.\nTo create containers and download image it\u0026rsquo;s needed create the default storage:\n$\u0026gt; lxc storage create default btrfs size=20GB In the example it used BTRFS loopback mode but there are different options.\nIf you setup LXD instance with lxd init --preseed you can setup some configurations directly with a YAML source.  Step 1b: Setup LXD instance through lxd-compose #  On both Macaroni OS and Funtoo it is possible to use lxd-compose to setup LXD instance directly with the project lxd-setup-and-test from LXD Compose Galaxy Project.\nNew OS will be integrated in the future.\nHereinafter, the steps to follow.\n  Install lxd-compose, jq and yq tools. Follow these commands or just install from your Linux distro the same tools:\n$\u0026gt; sudo su $\u0026gt; curl https://raw.githubusercontent.com/geaaru/luet/geaaru/contrib/config/get_luet_root.sh | sh # Install luet on your system $\u0026gt; luet install -y app-emulation/lxd-compose utils/jq utils/yq # Install lxd-compose binary   Get the LXD Compose Galaxy project\n$\u0026gt; git clone https://github.com/MottainaiCI/lxd-compose-galaxy.git $\u0026gt; cd lxd-compose-galaxy   Choice the storage type between the available visible with this command:\n$\u0026gt; lxd-compose storage list lxd-setup-and-test | STORAGES | DRIVER | DOCUMENTATION | |----------------|--------|---------------------------------------------------------------| | dir-source | dir | Directory Storage Pool using existing path /lxd. | | | | | | | | Using --render-env \u0026#34;storage_source=/mydir\u0026#34; to | | | | override existing /lxd path. | | | | | | btrfs-source | btrfs | BTRFS Storage Pool using existing path or device. | | | | | | | | Using --render-env \u0026#34;storage_source=/dev/sdX\u0026#34; to override | | | | default /lxd path. | | | | | | btrfs-loopback | btrfs | BTRFS Storage Pool Loop disk. | | | | | | | | Use --render-env \u0026#34;storage_size=200GB\u0026#34; to override | | | | default 150GB size. | | | | | | | | Use --render-env \u0026#34;storage_mount_options=compress=zstd:3\u0026#34; | | | | to enable additional btrfs mount options for compression | | | | or other. | | | | | | | | Other compressions options: | | | | | | | | storage_mount_options=rw,relatime,space_cache,compress=zstd:3 | | | | | | zfs-source | zfs | ZFS Storage pool using existing ZFS Pool or dataset | | | | or create a new ZFS Zpool on the specified device. | | | | | | zfs-loopback | zfs | ZFS Loopback Storage pool. | | | | | | | | Use --render-env \u0026#34;zfs_pool_name=pool\u0026#34; to override | | | | default ZFS pool name lxd-compose-pool. | | | | | | | | Use --render-env \u0026#34;storage_size=XGB\u0026#34; to override | | | | default storage size of 100GB | | | | | | lvm-source | lvm | LVM Storage Pool using existing path or device. | | | | | | | | Using --render-env \u0026#34;storage_source=/dev/sdx\u0026#34; to | | | | override existing `lvm` volume group. | | | | | | lvm-loopback | lvm | LVM Loopback Storage pool. | | | | | | ceph-source | ceph | Ceph Storage pool. | | | | | | | | NOTE: Not tested. | | | | | The description of the storage could not be describe all possibilies render options. After the identification of the storage type it\u0026rsquo;s better check the storage specification to see all possible options directly on repository.\n  When the storage is selected it\u0026rsquo;s needed create the render values file with the storage options. In the example the storage selected is btrfs-loopback with a size of 30GB and with compres:\n$\u0026gt; echo \u0026#34; storage_mount_options: compress=zstd:3 storage_size: 30GB storage_name: btrfs-loopback storage_render_values: render/values.yaml \u0026#34; \u0026gt; render/values.yaml The last row is needed because the lxd-compose specs executes the lxd-compose storage create command after the setup of LXD instance.\n  Define the configuration file of your LXD instance with the editing of the file envs/lxd-setup/vars/common.yaml.\nBetween the configuration options it\u0026rsquo;s possible define what are the users and the subuid/subgid to use for the management of the unprivileged containers and that will be configured by the execution:\n# Define the subuid/subguid to create lxc_subuids: - user: root home: /root map: 1000000-1065535 lxc_subgids: - user: root home: /root map: 1000000-1065535 And then the LXD instance configuration:\n# LXD Instance configuration lxd_config: config: # Configure the LXD password core.trust_password: mysecret # It seems that for cluster it\u0026#39;s better define the # server interface used. core.https_address: \u0026#34;[::]:8443\u0026#34; # Configure HTTPS proxy to use # core.proxy_https: \u0026#34;http://192.168.10.1:8080\u0026#34; # Configure HTTP proxy to use # core.proxy_http: \u0026#34;http://192.168.10.1:8080\u0026#34; # Configure Proxy Ignore hosts # core.proxy_ignore_hosts # See https://linuxcontainers.org/lxd/docs/master/server/ # for all available options. # Number of days after which an unused cached remote # image will be flushed images.remote_cache_expiry: 5 # Whether to automatically update any image that # LXD caches # images.auto_update_cached: true # Interval in hours at which # to look for update to cached images # (0 disables it) # images.auto_update_interval: 1 The selected storage will be configured in the default profile of the LXD instance configured.\n  Now it\u0026rsquo;s time to configure your LXD server and to run lxd-compose in the host where you want to do the setup (so use scp if it isn\u0026rsquo;t your node or another tool).\nSo, just run this in Macaroni:\n$\u0026gt; lxd-compose a lxd-setup-and-test --render-env \u0026#34;os=macaroni\u0026#34; --render-values render/values.yaml or this in Funtoo:\n$\u0026gt; lxd-compose a lxd-setup-and-test --render-env \u0026#34;os=funtoo\u0026#34; --render-values render/values.yaml Based on the selected OS the lxd-compose will install the packages (with the right PMS) and then will configure the LXD instance, it will create the LXD profiles and then it will test the setup.\n  You can always setup more of one storage too just using directly the lxd-compose storage create command.  Step 2: Configure LXD client #  lxd-compose uses the LXD\u0026rsquo;s client engine to call LXD API. This means that you need configure your environment with a config.yml that works with lxc too.\nIn general, the lxc client follows this steps on found the configuration files with the remotes and certificates to use:\n  if it sets LXD_CONF variable env that it uses the path defined in the variable to search the config.yml and to read the certificates\n  if LXD_CONF variable is not set or the file $LXD_CONF/config.yml doesn\u0026rsquo;t exist it search under $HOME/.config/lxc.\n  if $HOME/.config/lxc doesn\u0026rsquo;t exist it creates the directory and the file $HOME/.config/lxc/config.yml with the default remotes.\n  In additional, there are some extra variables available that are described here.\nBy default lxc uses the default remote local through LXD unix socket:\ndefault-remote: local remotes: images: addr: https://images.linuxcontainers.org protocol: simplestreams public: true local: addr: unix:// public: false It\u0026rsquo;s not possible to remove the local remote, it\u0026rsquo;s automatically added by LXD engine. To disable it (normally for the P2P Mode) you need set the field lxd_local_disable at true in the general section of lxd-compose config.  I think that a best practices to follow with lxd-compose (if it isn\u0026rsquo;t used the $HOME configuration file) is to create an lxd-conf directory where to register the needed remotes of the lxd-compose projects and the certificates.\nHereinafter, an example of the tree created with these steps:\n$\u0026gt; # Create your projects directory $\u0026gt; mkdir -p $HOME/lxd-compose-projects/my-first-project $\u0026gt; cd $HOME/lxd-compose-projects/my-first-project $\u0026gt; # Create lxd-conf directory to use project LXD configuration. $\u0026gt; mkdir lxd-conf $\u0026gt; # Force `lxc` client to use project directory $\u0026gt; # NOTE: Setting LXD_CONF variable is needed only for lxc client, $\u0026gt; # lxd-compose permits to avoid this with the specific option $\u0026gt; # descibed below. $\u0026gt; export LXD_CONF=./lxd-conf $\u0026gt; # Add remote related to the instance to use for my projects. $\u0026gt; # In this case we use the same node to use lxd-compose and LXD instance. $\u0026gt; lxc remote add mylxd-instance https://127.0.0.1:8443 --accept-certificate To start your first instance, try: lxc launch ubuntu:18.04 Generating a client certificate. This may take a minute... Admin password for mylxd-instance: $\u0026gt; find lxd-conf/ lxd-conf/ lxd-conf/client.key lxd-conf/servercerts lxd-conf/servercerts/mylxd-instance.crt lxd-conf/config.yml lxd-conf/client.crt In the example, the remote is an external host, if you want to use directly the local remote you just need to run lxc one time to create the right tree. Instead, if you want to use the LXD HTTP API locally you need to add the remote related to 127.0.0.1 address.\nWhen lxc client is been configured you are ready to setup lxd-compose config of the project. You can test it with:\n$\u0026gt; lxc list mylxd-instance: If it works, compliments the step 1 is completed!\nLXD installed from snapd #  LXD available through snapd doesn\u0026rsquo;t expose local unix socket under default path /var/lib/lxd/unix.socket but normally under the path /var/snap/lxd/common/lxd/unix.socket.\nThis means that to use local connection it\u0026rsquo;s better to create under the config.yaml an entry like this:\nlocal-snapd: addr: unix:///var/snap/lxd/common/lxd/unix.socket public: false and then to use local-snapd in connection option.\nWith LXD \u0026gt;=5.0 that contains new features to create multiple file socket with different permissions this configuration doesn\u0026rsquo;t work correctly. I will update the documentation correctly soon.  At the moment on Ubuntu the lxc command is supplied through snapd and it seems that that wrapper doesn\u0026rsquo;t propagate the environments variables to real binary. This means that override LXD_CONF doesn\u0026rsquo;t work. The LXD remotes must be modified only in the snapd path. Personally, I prefer to use OS where lxc is supplied as binary directly.  Instead, if it\u0026rsquo;s used the HTTPS API this is not needed.\nStep 3: Configure LXD Compose configuration file #  By default lxd-compose search for the file $PWD/.lxd-compose.yml else it is possible to pass the configuration file path with the -c|--config option.\nIn this case we override the LXD configuration path so what we need to do is to create a configuration file like this where we set also the path where lxd-compose search for the environment files.\n$\u0026gt; echo \u0026#34; general: debug: false lxd_confdir: ./lxd-conf logging: level: \u0026#34;info\u0026#34; # Define the directories list where lxd-compose search # for environments files with .yml or .yaml extension. env_dirs: - ./envs\u0026#34; \u0026gt; .lxd-compose.yml Step 4: Configure LXD profiles and networks #  This step is needed only if you want to define custom profiles to use with your containers and a particolar network configuration.\nIf your profiles are been defined like your network configuration you can skip this step.\nIf you have used the step 1b a lot of profiles and a network is already available and configured.\nUnder the LXD Compose Galaxy project, you can find a lot of examples with ready to use profiles that you can add to your project easily.  The next step is to create the project environment file:\n$\u0026gt; mkdir envs/ $\u0026gt; echo \u0026#34; version: \\\u0026#34;1\\\u0026#34; # Using mottainai template engine template_engine: engine: \\\u0026#34;mottainai\\\u0026#34; \u0026#34; \u0026gt; envs/env1.yml Prepare your profiles #  Now we can define our profiles inside the file envs/env1.yml after the content created above:\nprofiles: - name: \u0026#34;privileged\u0026#34; config: security.privileged: \u0026#34;true\u0026#34; description: Privileged profile devices: fuse: path: /dev/fuse type: unix-char tuntap: path: /dev/net/tun type: unix-char # Comment this if zfs is not available. zfs: path: /dev/zfs type: unix-char - name: \u0026#34;net-mottainai0\u0026#34; description: Net mottainai0 devices: eth0: name: eth0 nictype: bridged parent: mottainai0 type: nic - name: default description: Default Storage root: path: / pool: default type: disk - name: flavor-medium description: \u0026#34;flavor with 2GB RAM\u0026#34; config: limits.memory: 2GB - name: flavor-big description: \u0026#34;flavor with 3GB RAM\u0026#34; config: limits.memory: 3GB - name: flavor-thin description: \u0026#34;flavor with 500MB RAM\u0026#34; config: limits.memory: 500MB To configure profiles in the LXD instance you need define at least one project and one group. We need to add in the file envs/env1.yml this section:\nprojects: - name: \u0026#34;my-first-project\u0026#34; description: \u0026#34;This is my fist project.\u0026#34; groups: - name: \u0026#34;my-first-group\u0026#34; description: \u0026#34;My first group\u0026#34; # Set the remote to use for the group. # In this case we use the remote mylxd-instance # created in the previous steps. connection: \u0026#34;mylxd-instance\u0026#34; # We use the ephemeral container for this tutorial. ephemeral: true Now we are ready to initialize the LXD instance with our profiles:\n$\u0026gt; lxd-compose profile create my-first-project -a Profile privileged created correctly. Profile net-mottainai0 created correctly. Profile default already created correctly. Profile flavor-medium created correctly. Profile flavor-big created correctly. Profile flavor-thin created correcly. $\u0026gt; # Update existing profiles $\u0026gt; lxd-compose profile create my-first-project -a -u Prepare your networks #  Also the network devices used by LXD could be configured from lxd-compose and prepare the network with the right options used by the project.\nIn the file envs/env1.yml under the profiles section add the networks section:\nnetworks: - name: \u0026#34;mottainai0\u0026#34; type: \u0026#34;bridge\u0026#34; config: bridge.driver: native dns.domain: mottainai.local dns.mode: managed ipv4.address: 172.18.10.1/23 ipv4.dhcp: \u0026#34;true\u0026#34; ipv4.firewall: \u0026#34;true\u0026#34; ipv4.nat: \u0026#34;true\u0026#34; ipv6.nat: \u0026#34;false\u0026#34; ipv6.dhcp: \u0026#34;false\u0026#34; Then we create the network:\n$ lxd-compose network create my-first-project -a Network mottainai0 created. # To update configuration existing networks $ lxd-compose network create my-first-project -a -u You can check the result with:\n$ lxc network list +-----------------+----------+---------+----------------+---------------------------+-------------------------------------------+---------+ | NAME | TYPE | MANAGED | IPV4 | IPV6 | DESCRIPTION | USED BY | +-----------------+----------+---------+----------------+---------------------------+-------------------------------------------+---------+ | eth0 | physical | NO | | | | 0 | +-----------------+----------+---------+----------------+---------------------------+-------------------------------------------+---------+ | eth1 | physical | NO | | | | 0 | +-----------------+----------+---------+----------------+---------------------------+-------------------------------------------+---------+ | mottainai0 | bridge | YES | 172.18.10.1/23 | fd42:30e5:6279:93f::1/64 | Network mottainai0 created by lxd-compose | 0 | +-----------------+----------+---------+----------------+---------------------------+-------------------------------------------+---------+ There are a lot of options for setup network devices described in the LXD Project.\nStep 5: Create your first project #  We are ready to prepare our first deploy with lxd-compose. In this tutorial, we use single file specs for simplicity. My suggestion is to use the include_env_files and include_groups_files when there are verbose specs.\nWe need open the file envs/env1.yml again and to complete our group specs:\n- name: \u0026#34;my-first-project\u0026#34; description: \u0026#34;This is my fist project.\u0026#34; # We use online variable vars: - envs: yq_version: \u0026#34;3.4.1\u0026#34; ntpd_config: |# Pools for Gentoo users server 0.gentoo.pool.ntp.org server 1.gentoo.pool.ntp.org server 2.gentoo.pool.ntp.org server 3.gentoo.pool.ntp.org restrict default nomodify nopeer noquery limited kod restrict 127.0.0.1 restrict [::1] # In this case we have only one group and only # one project. Setting hooks here means that # are executed by all groups and nodes hooks: - event: \u0026#34;post-node-creation\u0026#34; commands: - apt-get update - apt-get upgrade -y - apt-get install -y htop vim jq ntp wget - event: post-node-creation commands: - sleep 2 - wget -q -O /usr/bin/yq https://github.com/mikefarah/yq/releases/download/${yq_version}/yq_linux_amd64 - chmod a+x /usr/bin/yq groups: - name: \u0026#34;my-first-group\u0026#34; description: \u0026#34;My first group\u0026#34; # Set the remote to use for the group. # In this case we use the remote mylxd-instance # created in the previous steps. connection: \u0026#34;mylxd-instance\u0026#34; # We use the ephemeral container for this tutorial. ephemeral: true common_profiles: - net-mottainai0 - sdpool nodes: - name: \u0026#34;node1\u0026#34; image_source: \u0026#34;ubuntu/20.10\u0026#34; image_remote_server: \u0026#34;images\u0026#34; entrypoint: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; hooks: - event: \u0026#34;post-node-sync\u0026#34; flags: - upgrade commands: - apt-get update \u0026amp;\u0026amp; apt-get upgrade -y - event: \u0026#34;post-node-sync\u0026#34; flags: - update_ntpd_config commands: - | echo \u0026#34;${ntpd_config}\u0026#34; \u0026gt; /etc/ntp.conf - cat /etc/ntp.conf - systemctl restart ntp Deploy your NTP Service project:\n# Create the container $\u0026gt; lxd-compose apply my-first-project Apply project my-first-project Searching image: ubuntu/20.10 For image ubuntu/20.10 found fingerprint 4aeac41e7f72b587409493fb814522fcf0925ba9c0c7250ab15fb36867c43d85 \u0026gt;\u0026gt;\u0026gt; Creating container node1... - üè≠ \u0026gt;\u0026gt;\u0026gt; [node1] - [started] üí£ \u0026gt;\u0026gt;\u0026gt; [node1] - apt-get update - ‚òï Hit:1 http://archive.ubuntu.com/ubuntu groovy InRelease Get:2 http://security.ubuntu.com/ubuntu groovy-security InRelease [110 kB] Get:3 http://archive.ubuntu.com/ubuntu groovy-updates InRelease [115 kB] Get:4 http://security.ubuntu.com/ubuntu groovy-security/main amd64 Packages [221 kB] Get:5 http://security.ubuntu.com/ubuntu groovy-security/universe amd64 Packages [57.6 kB] Get:6 http://archive.ubuntu.com/ubuntu groovy-updates/main amd64 Packages [366 kB] Get:7 http://archive.ubuntu.com/ubuntu groovy-updates/main Translation-en [94.1 kB] Get:8 http://archive.ubuntu.com/ubuntu groovy-updates/universe amd64 Packages [138 kB] Get:9 http://archive.ubuntu.com/ubuntu groovy-updates/universe Translation-en [53.2 kB] Fetched 1156 kB in 1s (1515 kB/s) Reading package lists... \u0026gt;\u0026gt;\u0026gt; [node1] - apt-get upgrade -y - ‚òï Reading package lists... Building dependency tree... Reading state information... Calculating upgrade... 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. \u0026gt;\u0026gt;\u0026gt; [node1] - apt-get install -y htop vim jq ntp wget - ‚òï Reading package lists... Building dependency tree... Reading state information... ... \u0026gt;\u0026gt;\u0026gt; [node1] - sleep 2 - ‚òï \u0026gt;\u0026gt;\u0026gt; [node1] - wget -q -O /usr/bin/yq https://github.com/mikefarah/yq/releases/download/${yq_version}/yq_linux_amd64 - ‚òï \u0026gt;\u0026gt;\u0026gt; [node1] - chmod a+x /usr/bin/yq - ‚òï \u0026gt;\u0026gt;\u0026gt; [node1] - apt-get update \u0026amp;\u0026amp; apt-get upgrade -y - ‚òï Hit:1 http://security.ubuntu.com/ubuntu groovy-security InRelease Hit:2 http://archive.ubuntu.com/ubuntu groovy InRelease Hit:3 http://archive.ubuntu.com/ubuntu groovy-updates InRelease Reading package lists... Reading package lists... Building dependency tree... Reading state information... Calculating upgrade... 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. \u0026gt;\u0026gt;\u0026gt; [node1] - echo \u0026#34;${ntpd_config}\u0026#34; \u0026gt; /etc/ntp.conf - ‚òï \u0026gt;\u0026gt;\u0026gt; [node1] - cat /etc/ntp.conf - ‚òï # Pools for Gentoo users server 0.gentoo.pool.ntp.org server 1.gentoo.pool.ntp.org server 2.gentoo.pool.ntp.org server 3.gentoo.pool.ntp.org restrict default nomodify nopeer noquery limited kod restrict 127.0.0.1 restrict [::1] \u0026gt;\u0026gt;\u0026gt; [node1] - systemctl restart ntp - ‚òï All done. If the node1 is already present lxd-compose skips post-node-creation hooks:\n$\u0026gt; lxd-compose apply my-first-project Apply project my-first-project \u0026gt;\u0026gt;\u0026gt; [node1] - apt-get update \u0026amp;\u0026amp; apt-get upgrade -y - ‚òï Hit:1 http://archive.ubuntu.com/ubuntu groovy InRelease Hit:2 http://archive.ubuntu.com/ubuntu groovy-updates InRelease Hit:3 http://security.ubuntu.com/ubuntu groovy-security InRelease Reading package lists... Reading package lists... Building dependency tree... Reading state information... Calculating upgrade... 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. \u0026gt;\u0026gt;\u0026gt; [node1] - echo \u0026#34;${ntpd_config}\u0026#34; \u0026gt; /etc/ntp.conf - ‚òï \u0026gt;\u0026gt;\u0026gt; [node1] - cat /etc/ntp.conf - ‚òï # Pools for Gentoo users server 0.gentoo.pool.ntp.org server 1.gentoo.pool.ntp.org server 2.gentoo.pool.ntp.org server 3.gentoo.pool.ntp.org restrict default nomodify nopeer noquery limited kod restrict 127.0.0.1 restrict [::1] \u0026gt;\u0026gt;\u0026gt; [node1] - systemctl restart ntp - ‚òï All done. And you can skip hooks:\n$\u0026gt; lxd-compose apply my-first-project --disable-flag upgrade Apply project my-first-project \u0026gt;\u0026gt;\u0026gt; [node1] - echo \u0026#34;${ntpd_config}\u0026#34; \u0026gt; /etc/ntp.conf - ‚òï \u0026gt;\u0026gt;\u0026gt; [node1] - cat /etc/ntp.conf - ‚òï # Pools for Gentoo users server 0.gentoo.pool.ntp.org server 1.gentoo.pool.ntp.org server 2.gentoo.pool.ntp.org server 3.gentoo.pool.ntp.org restrict default nomodify nopeer noquery limited kod restrict 127.0.0.1 restrict [::1] \u0026gt;\u0026gt;\u0026gt; [node1] - systemctl restart ntp - ‚òï All done. Destroy the project #  $\u0026gt; lxd-compose destroy my-first-project \u0026gt;\u0026gt;\u0026gt; [node1] - [stopped] ‚úî All done. Now you are ready to create your project! If it could be something that help people you can share it with the community through a PR at LXD Compose Galaxy.  "}),a.add({id:2,href:'/lxd-compose-docs/docs/vmware-1-1/',title:"Vmware 1 1",section:"LXD Compose",content:"VmWare-LXD 1:1 #  In the Kata Containers technology it\u0026rsquo;s used an Hardware Virtualization to supply an additional isolation with a lightweight VM and individual kernels.\nIn a similar way the use case describe here try to use a single Linux VM over VmWare where install a standalone LXD instance and then through lxd-compose deploy one or more services using a Physical vNic that is managed by LXD and added from the VM to the Container deployed.\n In this scenario to supply the classic HA services it\u0026rsquo;s needed follow exactly the steps that normally are follow on delivery HA service directly over VMs. This means deploy multiple VMs with the same services (for example two nodes for Nginx Server) and eventually using VIPs.  As visible in the image every VM is configured with two differents vNICs.\nA management vNic/Iface that is only available over the Host/VM and it\u0026rsquo;s used to communicate with LXD HTTPS API (normally over the 8443 port) and/or for SSH access (VM packages upgrades, maintenance, etc.). It\u0026rsquo;s a good idea to reach this interface only over a private VPN.\nA service vNic/iface that is used over the container to supply services configured in the container.\nTo ensure a more clean setup of the host a best practices is to rename the network\u0026rsquo;s interface with a name more oriented with the target infrastructure. In the example, it\u0026rsquo;s used the name srv0 defined in the LXD profile assigned to the container to deploy. The same could be done for the management interface with a name like man0.\nTo rename the network interfaces it\u0026rsquo;s needed create an udev rule based on the MAC addresses assigned from VmWare to the VM. For example, editing the file /etc/udev/rules.d/70-persistent-net.rules:\nSUBSYSTEM==\u0026quot;net\u0026quot;, ACTION==\u0026quot;add\u0026quot;, DRIVERS==\u0026quot;?*\u0026quot;, ATTR{address}==\u0026quot;XX:XX:XX:XX:XX:XX\u0026quot;, ATTR{dev_id}==\u0026quot;0x0\u0026quot;, ATTR{type}==\u0026quot;1\u0026quot;, KERNEL==\u0026quot;eth*\u0026quot;, NAME=\u0026quot;man0\u0026quot; SUBSYSTEM==\u0026quot;net\u0026quot;, ACTION==\u0026quot;add\u0026quot;, DRIVERS==\u0026quot;?*\u0026quot;, ATTR{address}==\u0026quot;XX:XX:XX:XX:XX:YY\u0026quot;, ATTR{dev_id}==\u0026quot;0x0\u0026quot;, ATTR{type}==\u0026quot;1\u0026quot;, KERNEL==\u0026quot;eth*\u0026quot;, NAME=\u0026quot;srv0\u0026quot; and then without the needed of reboot the VM just to execute:\n$\u0026gt; # Ensure that the selected interface are down $\u0026gt; ip link set eth0 down $\u0026gt; ip link set eth1 down $\u0026gt; udevadm control -R \u0026amp;\u0026amp; udevadm trigger --action=add -v -s net When the VMs are all configured with LXD reachable over HTTPS the lxd-compose tool will reach every nodes through the different remotes. The file lxd-conf/config.yml will be configured in this way:\ndefault-remote: local remotes: images: addr: https://images.linuxcontainers.org protocol: simplestreams public: true macaroni: addr: https://images.macaroni.funtoo.org/lxd-images protocol: simplestreams public: true myserver: addr: https://mysimplestreams.server.local/lxd-images protocol: simplestreams public: true local: addr: unix:// public: false local-snapd: addr: unix:///var/snap/lxd/common/lxd/unix.socket public: false vm1: addr: https://vm1.infra:8443 auth_type: tls project: default protocol: lxd public: false vm2: addr: https://vm2.infra:8443 auth_type: tls project: default protocol: lxd public: false aliases: {} The remote vm1 and vm2 are the remotes of the VMs where deploy the target services.\nAs described in the schema over the LXD instances is configured a LXD profile that map the VM vNIC srv0 of the VM to the container with the same name. The nictype is physical is the type implemented by LXD to support this feature.\nUsing native bridge or OpenVswitch bridge attached to a VmWare vNic is something that requires a specific setup over VmWare because the vSwitch uses MAC caching and MAC filter that often doesn\u0026rsquo;t permit to have something that work correctly. For this reason and because not always the people that implement and deploy the services are the same that control the VmWare the uses of the physical interface over the container is the best solution to follow because it\u0026rsquo;s transparent to VmWare. The MAC address assigned to the container is the same mapped over VmWare. An alternative is to use the port map between the LXD containers and the VM vNIC but it\u0026rsquo;s another use case describe later.\nIn the example, it\u0026rsquo;s been used only one network interface to supply services but there aren\u0026rsquo;t limitations on this. Could be configured multiple vNics at VmWare level that are assigned to the containers, for example to supply an SSH service through a management interface directly in the container.\nFor this solution it\u0026rsquo;s a good idea to prepare a VmWare VM Template that is used to create new VMs to attach to the service when it\u0026rsquo;s needed to scale or just upgrade the OS and reduce the offline time with an upgrade over a VM not attached to the running services.\nDynamic nodes with the LXD Compose render engine #  Before describe how could be possible using the render engine to define the lxd-compose specifications to manage the service setup I want remember to the reader that a specific connection with an LXD instance could be defined only at group level. It\u0026rsquo;s possible that multiple groups could be connected to the same LXD instance but it\u0026rsquo;s not true that the same group could be used to control multiple LXD instance at the same time (excludes the use case with LXD Cluster not covered by this scenario). Based on this concept a defined group is mapped to a specific VM and on using this scenario a solution that works pretty well it\u0026rsquo;s the use of the render engine to specify the list of the remotes (or LXD instance) to reach for a specific service following the pattern described hereinafter.\nThe render file will supply the list of the remotes assigned for a specific service, for example the NGinx servers that exposes the HTTP resources. The render file is defined directly over the .lxd-compose.yml or --render-values|--render-default options.\n$ cat .lxd-compose.yml general: lxd_confdir: ./lxd-conf render_default_file: ./render/default.yaml render_values_file: ./render/prod.yaml env_dirs: - ./envs/ in this case are defined both default render and the values render.\nThe prod.yaml file could be configured in this way:\nrelease: \u0026#34;22.10\u0026#34; nginx_nodes: - connection: \u0026#34;vm1\u0026#34; name: \u0026#34;nginx1\u0026#34; - connection: \u0026#34;vm2\u0026#34; name: \u0026#34;nginx2\u0026#34; And these could be the options defined in the default.yaml:\nephemeral: false privileged_containers: false The connections vm1 and vm2 are the remotes described before and defined in the config.yml file.\nThis could be an example of the LXD Compose specs where is defined the setup of the Nginx server.\n# Description: Setup the Nginx Production Service version: \u0026#34;1\u0026#34; include_storage_files: - common/storages/default.yml include_profiles_files: - common/profiles/default.yml - common/profiles/net-phy-srv.yml - common/profiles/logs-disk.yml - common/profiles/autostart.yml projects: - name: \u0026#34;nginx-services\u0026#34; description: | Setup Nginx services. include_env_files: - myenv/vars/common.yml # Include the variable file that contains # the network configurations options. {{ range $k, $v := .Values.nginx_nodes }} - myenv/vars/{{ $v.name }}-net.yml {{ end }} groups: {{ $groups := .Values.nginx_nodes }} {{ $ephemeral := .Values.ephemeral }} {{ range $k, $v := .Values.nginx_nodes }} - name: \u0026#34;{{ $v.name }}-group\u0026#34; description: \u0026#34;Setup Nginx Frontend Node {{ $v.name }}\u0026#34; connection: \u0026#34;{{ $v.connection }}\u0026#34; common_profiles: - default - net-phy-srv - logs-disk {{- if $privileged_containers }} - privileged {{ end }} - autostart ephemeral: {{ $ephemeral }} include_hooks_files: - common/hooks/systemd-net-static.yml - common/hooks/systemd-dns.yml - common/hooks/hosts.yml nodes: - name: {{ $v.name }} image_source: \u0026#34;nginx/{{ $.Values.release }}\u0026#34; image_remote_server: \u0026#34;myserver\u0026#34; hooks: - event: post-node-sync flags: - config commands: - \u0026gt;-systemctl daemon-reload \u0026amp;\u0026amp; systemctl enable nginx \u0026amp;\u0026amp; systemctl restart nginx # Generate the nginx configuration file based on template # and project variables. config_templates: - source: nginx/templates/nginx.tmpl dst: /tmp/lxd-compose/myinfra/{{ $v.name }}/nginx.conf sync_resources: - source: /tmp/lxd-compose/myinfra/{{ $v.name }}/nginx.conf dst: /etc/nginx/ {{ end }} Some helpful commands to analyze the specs are these:\n$\u0026gt; lxd-compose project list | PROJECT NAME | DESCRIPTION | # GROUPS | |-------------------|-------------------------------------------------------------------------|----------| | nginx-services | Setup Nginx Services | 2 | $\u0026gt; lxd-compose group list nginx-services | GROUP NAME | DESCRIPTION | # NODES | |----------------|--------------------------------------|---------| | nginx1-group | Setup Nginx Frontend Node nginx1 | 1 | | nginx2-group | Setup Nginx Frontend Node nginx2 | 1 | Deploy the service #  The first time the all containers are down it\u0026rsquo;s possible just deploy all nodes with the apply command:\n$\u0026gt; lxd-compose apply nginx-services When you want upgrade a production service it\u0026rsquo;s a good practice replace one node a time in this way:\n$\u0026gt; # Destroy the node nginx1. $\u0026gt; lxd-compose destroy nginx-services --enable-group nginx1-group $\u0026gt; # Deploy the new container related to the new release 22.10.01. $\u0026gt; # The variable release could be passed in input or updated directly on default.yaml $\u0026gt; lxd-compose apply nginx-services --enable-group nginx1-group --render-env \u0026#34;release=22.10.01\u0026#34; The same could be done in for the second node when the first is up and running.\nIf you have a slow bandwitdh between the LXD images server could be helpful download the images over the VMs before execute the upgrade with the fetch command:\n$\u0026gt; lxd-compose fetch nginx-services This command will download the LXD images over the configured LXD instances of the selected project without destroy and/or create containers.\nIn the example it\u0026rsquo;s used the image LXD with alias nginx/\u0026lt;release\u0026gt;. This image is created and exposed over HTTPS in a separated step. If this is not available it\u0026rsquo;s possible to use the images available over Macaroni Simplestreams Server or Canonical Server and just install packages in the post-node-creation phase to prepare the container with all needed software.  If it\u0026rsquo;s needed add a new node/VM you need just editing the file prod.yaml and the config.yml to add the new node.\n$\u0026gt; cat lxd-conf/config.yml ... vm3: addr: https://vm3.infra:8443 auth_type: tls project: default protocol: lxd public: false $\u0026gt; cat render/prod.yml release: \u0026#34;22.10\u0026#34; nginx_nodes: - connection: \u0026#34;vm1\u0026#34; name: \u0026#34;nginx1\u0026#34; - connection: \u0026#34;vm2\u0026#34; name: \u0026#34;nginx2\u0026#34; - connection: \u0026#34;vm3\u0026#34; name: \u0026#34;nginx3\u0026#34; Again, if it\u0026rsquo;s needed update the configuration files of all Nginx servers this could be executed sequentially from lxd-compose with the same apply command.\nIn the reported example for every container is assigned a static IP address that is defined in one vars file following this pattern that display the file nginx1-net.yml:\nenvs: nginx1_resolved: - file: dns.conf content: |[Resolve] DNS=1.1.1.1 FallbackDNS=8.8.8.8 Domains= MulticastDNS=no LLMNR=no nginx1_hosts: # - ip: \u0026lt;ip\u0026gt; # domain: \u0026lt;domain1\u0026gt;,\u0026lt;domain2\u0026gt; - ip: \u0026#34;10.10.10.1\u0026#34; domain: nginx-backend01 - ip: \u0026#34;10.10.10.2\u0026#34; domain: nginx-backend02 nginx1_net: - file: 01-srv0.network content: |[Match] Name=srv0 [Network] Address=172.18.20.100/24 Gateway=172.18.20.1 LLMNR=no # Disable binding of port ::58 IPv6AcceptRA=no The hooks used for the configuration are systemd-net-static.yml, systemd-dns.yml and hosts.yml.\nUpgrade the OS of the VM #  Hereinafter a short description about what are the possibile steps to follow in Production to replace the VM and move an active container over a new VM where is been upgraded the OS.\nObviously, it\u0026rsquo;s possible upgrade the OS and the LXD instance without replacing the VM but we will try to share a way that could be used for rollback if something goes wrong.\nSo, the first step is to clone the existing VM or just create a new VM from a template. If the choice is cloning, just disable service network from VmWare before boostrap of the VM.\n The new VM with a specific management interface could be upgraded meantime the service is up and running. Until the network interface srv0 is not assigned to the container is visible in the VM.\nWhen the new VM is ready the steps to follow are describe in the image hereinafter:\n In short, through the lxd-compose tool it\u0026rsquo;s needed destroy the existing container. Eventually, you can just create a backup of the container with the normal lxc tool or copy it before run the destroy command.\n$\u0026gt; # Create the backup of the container for the rollback. $\u0026gt; lxc copy vm1:nginx1 vm1:nginx-bkp1 $\u0026gt; # Or just stop the existing container. $\u0026gt; lxc stop vm1:nginx1 If the container is stopped with lxc command the destroy command is not needed.\nWhen the container is stopped or destroyed, the service IP address 172.18.20.100 of the example will be reused in the deploy of the new container over the new VM. To maintain the same name of the container it\u0026rsquo;s only needed modify the prod.yml file to have vm1-new (or any other name choiced) that is the name of the remote of the new VM.\nSo, in the prod.yml the content become:\nrelease: \u0026#34;22.10\u0026#34; nginx_nodes: - connection: \u0026#34;vm1-new\u0026#34; name: \u0026#34;nginx1\u0026#34; - connection: \u0026#34;vm2\u0026#34; name: \u0026#34;nginx2\u0026#34; - connection: \u0026#34;vm3\u0026#34; name: \u0026#34;nginx3\u0026#34; After this change it\u0026rsquo;s only needed to execute the apply command that deploy the new container:\n$\u0026gt; lxd-compose apply --enable-group nginx1-group If something goes wrong and the user want rollback the previous container it\u0026rsquo;s needed: shutdown the new VM; if the container is been stopped and not destroyed, just rerun the apply command with the connection reconfigured to vm1.\nIf the container is been destroyed and it\u0026rsquo;s been used a production-ready LXD image the user could just redeploy the new container with the previous release.\nBest Practices for Production environments #  As all know, today all Linux distribution are Rolling Release for different reasons: the world go ahead very fast, CVE and security issues that are identified every day requires fast updates, etc. This means that what is deployed at time T0 at the 99% is not installable ad the same way at time T1. So, for production services it\u0026rsquo;s better to prepare LXD images that will be used on delivery without execute OS upgraded on container creation.\nThe upgrades of the container OS must be apply in a testing environment where it\u0026rsquo;s possible verify that there aren\u0026rsquo;t regression with a new LXD images that when the QA are passed could be used to upgrade production environment.\nThe Simplestreams Builder could be used to prepare an HTTPS endpoint where expose LXD images used in the installation over the Simplestreams Protocol. The alternative is to use directly another LXD Instance as LXD images supplier.\nFor my experience it works well to try using an environment specification with a single project to define a group of VMs that supply a specific service. This will simplify the upgrade process with the use of one group for every LXD instance as described in the previous chapter. Users are free to find their correct way. Having multiple projects defined in the same environment file it works too like to have a static definition of the groups without using render engine.\nUsing the physical nictype doesn\u0026rsquo;t permit to see the network interfaces from the VM OS, so it\u0026rsquo;s important to supply on the created LXD images the right tools for throubleshooting and analyses (tcpdump, ethtool, etc.).\nUsing a VmWare VM already supply a good isolation but to improve the security levels using the unprivileged containers is the best choice.\nIn a production environment often it\u0026rsquo;s important to maintain the logs files of exposed services for subsequent analyzes. This could be handled through the mount of the VM path inside the container that is persistent between the upgrade of one container with another but not if the VM is replaced. But there are different ways to resolve this efficiently for example through an Rsyslog remote server or over Vmware using a secondary disk that is detached and attached to the new VM after the upgrade. In the presented example the profile used for this mission is logs-disk.\nThe use of additional API to setup and create a VM through the VmWare API it\u0026rsquo;s out of scope of this guide. This doesn\u0026rsquo;t mean that is not possible. lxd-compose permits to define hooks pre-project related to the node host that means the local shell where lxd-compose is executed. Inside that hook it\u0026rsquo;s possible execute a bash script or other that prepare the VmWare VM before execute the delivery of the container.  "}),a.add({id:3,href:'/lxd-compose-docs/docs/vmware-1-n-forward/',title:"Vmware 1 N Forward",section:"LXD Compose",content:"VmWare-LXD 1:N #  A different approach is to use a single VM where configure a single LXD instance to deploy multiple containers and supply different services.\nIn these use cases, the VM could expose the services:\n  with a floating IP that is exposes over the VM network interface and the internal network is hidden. On this case it\u0026rsquo;s used the Network Forward feature of LXD.\n  through the PROXY protocol to reach an internal service without expose a direct access from external network and the internal service. In this case normally, it used a reverse balancer like Nginx.\n  through a NAT proxy device to reach an internal service with a specific device proxy resource.\n  through a proxy device on loopback of the device with a specific device proxy resource.\n  The official documentation about devices proxy is available here.  Floating IP from VM iface #  Between the features available in LXD and configurable through lxd-compose exists the Network Forwards that permit to define one or more Floating IP address configured in one or more network interfaces of the node that will be used to define network flows to redirect in the inside containers.\nOne of the way to understand how it works it through an example.\nIf we consider that the requirement is to expose two different TCP services over two different ports with a single Floating IP:\n  port 8081 for the service of the application A\n  port 8080 for the service of the application B\n  the graph hereinafter, describe the behavior:\n The IP address 192.168.10.10 is our floating IP that is reachable from the external network. In the example is used a Private Network IP but using a real Public IP is pretty the same.\nIn particular, the IP address 192.168.10.10 is assigned to the srv0 interface as primary ip address or as additional IP address.\n4: srv0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ee:ce:9f:0b:c8:7c brd ff:ff:ff:ff:ff:ff inet 192.168.10.10/24 brd 192.168.0.255 scope global noprefixroute srv0 valid_lft forever preferred_lft forever or\n4: srv0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether ee:ce:9f:0b:c8:7c brd ff:ff:ff:ff:ff:ff inet 192.168.0.1/24 brd 192.168.0.255 scope global noprefixroute srv0 valid_lft forever preferred_lft forever inet 192.168.10.10/24 scope global secondary srv0 valid_lft forever preferred_lft forever The floating IP is not managed by LXD and must be configured manually.\nThe internal network bridge is mottainai0 and is managed by LXD.\n$ lxc network show mottainai0 config: bridge.driver: native dns.domain: mottainai.local dns.mode: managed ipv4.address: 172.18.1.249/23 ipv4.dhcp: \u0026#34;true\u0026#34; ipv4.firewall: \u0026#34;true\u0026#34; ipv4.nat: \u0026#34;true\u0026#34; ipv6.dhcp: \u0026#34;false\u0026#34; ipv6.nat: \u0026#34;false\u0026#34; description: Network mottainai0 created by lxd-compose name: mottainai0 type: bridge used_by: - /1.0/instances/c1 - /1.0/instances/c2 - /1.0/profiles/net-mottainai0 managed: true status: Created locations: - none In the example it\u0026rsquo;s used the native bridge but it works with OVS bridge too.\nThe Application A is exposed in the internal container C2 over the port 8090 and with the IP 172.18.1.1.\nThe Application B is exposed in the internal container C1 over the port 8080 and with the IP 172.18.1.2.\nTo define Network Forward rules you need to know the IP addresses of the target containers. So you could create the containers and later create/update the forward or using static IP addresses in the containers.  lxd-compose simplify the management of the Network Forward more bloated through the lxc command.\nA network forward listen address must be assigned to an existing network and for this reason that the lxd-compose manage these resources as extension of the network object.\n# LXD Compose specs to define Network Forward over a network device. networks: - name: \u0026#34;mottainai0\u0026#34; type: \u0026#34;bridge\u0026#34; config: bridge.driver: native dns.domain: mottainai.local dns.mode: managed ipv4.address: 172.18.1.249/23 ipv4.dhcp: \u0026#34;true\u0026#34; ipv4.firewall: \u0026#34;true\u0026#34; ipv4.nat: \u0026#34;true\u0026#34; ipv6.nat: \u0026#34;false\u0026#34; ipv6.dhcp: \u0026#34;false\u0026#34; forwards: - listen_address: \u0026#34;192.168.10.10\u0026#34; ports: - protocol: tcp # Define a port or a port-range or a list of port. listen_port: \u0026#34;8081\u0026#34; target_address: \u0026#34;172.18.1.1\u0026#34; target_port: \u0026#34;8090\u0026#34; - protocol: tcp listen_port: \u0026#34;8080\u0026#34; target_address: \u0026#34;172.18.1.2\u0026#34; So, after define the network specifications and the forwards rules in the target environment just create them with lxd-compose:\n$ lxd-compose network create myproject mottainai0 --with-forwards -u Network mottainai0 updated. Network forwards of the net mottainai0 updated. With the same command it\u0026rsquo;s possible update the existing rules.\nIndeed, the commands of lxc tool to run to check the configuration are:\n$ lxc network forward list mottainai0 +----------------+-------------------------------------------------------------+------------------------+-------+ | LISTEN ADDRESS | DESCRIPTION | DEFAULT TARGET ADDRESS | PORTS | +----------------+-------------------------------------------------------------+------------------------+-------+ | 192.168.10.10 | Network forward for ip 192.168.10.10 created by lxd-compose | | 2 | +----------------+-------------------------------------------------------------+------------------------+-------+ And this to see the detail of a specific listen address:\n$ lxc network forward show mottainai0 192.168.10.10 description: Network forward for ip 192.168.10.10 created by lxd-compose config: {} ports: - description: \u0026#34;\u0026#34; protocol: tcp listen_port: \u0026#34;8081\u0026#34; target_port: \u0026#34;8090\u0026#34; target_address: 172.18.1.1 - description: \u0026#34;\u0026#34; protocol: tcp listen_port: \u0026#34;8080\u0026#34; target_address: 172.18.1.2 listen_address: 192.168.10.10 location: none Under the hood LXD uses iptables or nftables to define a DNAT rule that permit to maintain the origin source IP address when the connection reachs the internal node and a MASQUERADE rule for the revert flow.\n$ sudo iptables -t nat -L -n Chain PREROUTING (policy ACCEPT) target prot opt source destination DNAT tcp -- 0.0.0.0/0 192.168.10.10 tcp dpt:8080 /* generated for LXD network-forward mottainai0 */ to:172.18.1.2:8080 The use of target_port attribute is needed only when the public listening ports are different from the internal else you can just define the listen_port attribute.\nThe official LXD documentation of the Network Forward feature is available here.\nUsing PROXY protocol to reach an internal service #  LXD permits to define proxy devices to allow forwarding network connections between host and instance. This makes it possible to forward traffic hitting one of the host‚Äôs addresses to an address inside the instance or to do the reverse and have an address in the instance connect through the host.\nBetween the different types of connections (udp, tcp, unix) it\u0026rsquo;s possible forward the connection encapsuled over the PROXY protocol that transmit the sender information.\nUsing PROXY protocol like this example:\nname: \u0026#34;mottainai-https\u0026#34; description: \u0026#34;Profile for export HTTPS port to Host\u0026#34; devices: https: bind: host connect: tcp:0.0.0.0:443 listen: tcp:0.0.0.0:443 nat: false proxy_protocol: true type: proxy permits to avoid using a static IP address inside the container.\nIn this case, it\u0026rsquo;s a LXD process that execute the binding of the port and proxy the connection inside the container.\n# # From the VM / Host # netstat -lpn | grep lxd | grep 443 tcp6 0 0 :::443 :::* LISTEN 1229/lxd Using NAT proxy device to reach an internal service #  Always through the use of device proxy when the nat option is enable the traffic is forwarded usint NAT than being proxied through a separate connection, in this case you need ensure that the target instance has a static IP configured in LXD on its NIC device.\nTo have a static IP address you need to configure the NIC device with a configuration for the node similar to this:\ndevices: eth0: ipv4.address: 172.18.1.1 name: eth0 nictype: bridged parent: mottainai0 type: nic myservice: bind: host connect: tcp:172.18.1.1:11000 listen: tcp:192.168.10.10:11000 nat: \u0026#34;true\u0026#34; proxy_protocol: \u0026#34;false\u0026#34; type: proxy Without configure a static IP address for the eth0 device the bind with NAT fail.\nIn addition, when nat is true you can\u0026rsquo;t use listen with tcp:0.0.0.0 but you need define the external IP address where LXD will configure the NAT rule.\nI think that using this solution it makes sense when you have a very little installation, else the Network Forward way is better.\nUnder the hood, using the nat generates the configuration of these iptables rules:\n$ sudo iptables -t nat -L -n Chain PREROUTING (policy ACCEPT) target prot opt source destination DNAT tcp -- 0.0.0.0/0 192.168.10.10 tcp dpt:11000 /* generated for LXD container c2 (myservice) */ to:172.18.1.1:11000 Chain OUTPUT (policy ACCEPT) target prot opt source destination DNAT tcp -- 0.0.0.0/0 192.168.10.10 tcp dpt:11000 /* generated for LXD container c2 (myservice) */ to:172.18.1.1:11000 Chain POSTROUTING (policy ACCEPT) target prot opt source destination MASQUERADE tcp -- 172.18.1.1 172.18.1.1 tcp dpt:11000 /* generated for LXD container c2 (myservice) */ Using proxy device to reach an internal service on localhost #  If the requirements of the called service is not identify the calling it\u0026rsquo;s possible define a proxy device rule that map an external port to a specific container through his loopback interface.\nname: \u0026#34;myservice\u0026#34; description: \u0026#34;Profile for export port 12000 to Host\u0026#34; devices: https: bind: host connect: tcp:127.0.0.1:12000 listen: tcp:0.0.0.0:12000 nat: false proxy_protocol: false type: proxy In this case an LXD process is configured in binding on the specified port and the traffic is proxied inside the container in the selected port.\n# # From the VM / Host # netstat -lpn | grep lxd | grep 12000 tcp6 0 0 :::12000 :::* LISTEN 1229/lxd "}),a.add({id:4,href:'/lxd-compose-docs/docs/vmware/',title:"Vmware",section:"LXD Compose",content:"LXD Compose \u0026amp; VmWare #  The mission of this section is try to describe some use cases about using LXD Compose over a VMWare stack to supply production-ready services and help the systemist and developers life.\nIn particular, over my work experiences I saw a lot of different requirements exposed by my Clients. Just to help the reader I will try to describe in addition of the behavior of the specific use case what are the pro \u0026amp; cons.\nThe described use cases are:\n Vmware-LXD 1:1: this scenario follow the idea of the Kata Containers where are used the VMWare VMs as nodes where install LXD standalone instances where lxd-compose could be configured to delivery the services over 1 container for 1 VM.\n Vmware-LXD 1:N: this scenario it\u0026rsquo;s maybe the more used where inside a VM and a single LXD instance are started more of one container and with different means are exposed service over specific ports, etc.\n"}),a.add({id:5,href:'/lxd-compose-docs/docs/concepts/env/',title:"Environements",section:"Concepts",content:"Environments #  The environments are characterized of from these properties or elements:\n  version: describe the version of the specifications. At the moment the only version supported is \u0026ldquo;1\u0026rdquo;.\n  template_engine: under the template_engine section is configured the engine used for create project files. The available engines are jinja2 (it uses j2cli tool and jinja2 framework like for Ansible) and mottainai that it uses Golang\u0026rsquo;s template render. For the jinja2 template is possible set additional options for the j2cli through opts field.\n  profiles: the list of LXD profiles used by the environment that could be added and/or updated on the LXD instances used by the projects.\n  networks: the list of LXD networks used by the environment that could be added and/or updated on the LXD instances used by the projects.\n  commands: the list of commands defined related to the projects of the environment. It\u0026rsquo;s helpful to have a register of the more useful commands to run on a running system for backup, validation, etc. The commands are aliases of apply command where it\u0026rsquo;s possible define flags, additional hooks, etc.\n  storages: the list of LXD storages used by the environment that could be added and/or updated on the LXD instances used by the projects.\n  projects: the projects to deploy.\n  lxd-compose read all files under the directories defined on the paramater env_dirs of the configuration file and load the specification of all projects in memory before run commands.\nThe profiles, networks, storages and commands are all loaded and rendered through the LXD Compose render engine that permit to customize the entities without create multiple time the same resource but with few differences.\nHereinafter, an extract of the configuration file available on LXD Compose Galaxy:\ngeneral: debug: false # lxd_confdir: ./lxd-conf push_progressbar: false logging: level: \u0026#34;info\u0026#34; # Define the directories list where load # environments. env_dirs: - ./envs/nginx - ./envs/mottainai-server where is been defined the directories where lxd-compose the files envs/{nginx,mottainai-server}/*.yml or .yaml files.\nThe environment\u0026rsquo;s files could be a pure YAML file or template for the Helm engine; in this case, you need to define the render values file from CLI or from the configuration file.\nFor example you can define the source image used by a node inside a group in this way:\nnodes: - name: \u0026quot;node1\u0026quot; image_source: \u0026quot;alpine/{{ .Values.alpine_version }}\u0026quot; image_remote_server: \u0026quot;images\u0026quot; and to test your services with all available version of alpine images on define different render files like this:\n# file: alpine3_11.yml alpine_version: \u0026#34;3.11\u0026#34; # file: alpine3_10.yml alpine_version: \u0026#34;3.10\u0026#34; At this point you can run your project in this way:\n$\u0026gt; lxd-compose apply myproject --render-values alpine3_11.yml $\u0026gt; lxd-compose destroy myproject $\u0026gt; lxd-compose apply myproject --render-values alpine3_10.yml In alternative you can set the default render file inside the config:\n# file: .lxd-compose.yml render_default_file: alpine3_11.yml and then override the value only when it\u0026rsquo;s needed:\n$\u0026gt; lxd-compose apply myproject $\u0026gt; lxd-compose destroy myproject $\u0026gt; lxd-compose apply myproject --render-values alpine3_10.yml In general, the render engine is used to generate the environment\u0026rsquo;s files at runtime, instead the template engine defined inside the environment is used as template engine for the files to use inside the deploy workflow.  It‚Äôs a good practice avoid to use group names equal across different projects or nodes with equals names because inside the project it‚Äôs possible to define a hook to execute a command to an external node of the project. The lxd-compose validate command blocks duplicate at the moment.  Profiles #  Inside the environment\u0026rsquo;s files could be defined the LXD profiles:\n# Define the list of LXD Profiles used by all projects. # This profiles are not mandatory. An user could create # his profiles without to use this list. profiles: - name: \u0026#34;mottainai-https\u0026#34; description: \u0026#34;Profile for export HTTPS port to Host\u0026#34; devices: https: bind: host connect: tcp:0.0.0.0:443 listen: tcp:0.0.0.0:443 nat: false proxy_protocol: true type: proxy This is section is used only for tracing the profiles needed by the infrastructure. It is possible create and/or update profiles through the lxd-compose profile subcommand.\nThe definition of the profiles could be inline over the environment YAML or with external files through the include_profiles_files attribute.\nNetworks #  In a similar way, inside an environment file it\u0026rsquo;s possible define the list of network device or bridge used by the LXD instances.\nnetworks: - name: \u0026#34;mottainai0\u0026#34; type: \u0026#34;bridge\u0026#34; config: bridge.driver: native dns.domain: mottainai.local dns.mode: managed ipv4.address: 172.18.10.1/23 ipv4.dhcp: \u0026#34;true\u0026#34; ipv4.firewall: \u0026#34;true\u0026#34; ipv4.nat: \u0026#34;true\u0026#34; ipv6.nat: \u0026#34;false\u0026#34; ipv6.dhcp: \u0026#34;false\u0026#34; To show all possible configurations for both networks and profiles there is the LXD documentation. lxd-compose maps the API configurations directly.\nSome examples are available on LXD Compose Galaxy.\nThe definition of the networks could be inline over the environment YAML or with external files through the include_networks_files attribute.\nACLs #  It\u0026rsquo;s possible define traffic rules that allow controlling network access between different instances connected to the same network or other networks.\nThis could be done directly to the NICs of an instance or to a network.\nlxd-compose permits to trace the ACLs at environment level and then use them through the security.acls option in the device section of the container or of the network.\nAdditional details could be retrieved from the LXD documentation.\nAssign a security.acls directly to a NIC of a container is possible only for OVN networks.  Normally, the definition and the creation of the ACLs must be done before the networks because, a specific ACL could be assigned to a network in this way:\nnetworks: - name: \u0026#34;mottainai0\u0026#34; type: \u0026#34;bridge\u0026#34; config: bridge.driver: native dns.domain: mottainai.local dns.mode: managed ipv4.address: 172.18.1.249/23 ipv4.dhcp: \u0026#34;true\u0026#34; ipv4.firewall: \u0026#34;true\u0026#34; ipv4.nat: \u0026#34;true\u0026#34; ipv6.nat: \u0026#34;false\u0026#34; ipv6.dhcp: \u0026#34;false\u0026#34; security.acls: acltest Hereinafter, an example about how could be created an ACL with an ingress and egress rules:\nacls: - name: \u0026#34;acltest\u0026#34; ingress: - action: allow destination: 172.18.1.1,172.18.1.2 protocol: icmp4 state: enabled egress: - action: allow destination: 0.0.0.0/0 destination_port: 443 protocol: tcp state: enabled In the example the ACL acltest allow ingress traffic to 172.18.1.1 and 172.18.1.2 for ICMPv4 and allow traffic to all destination for the TCP/443 flows.\nWhen the ACLs are defined on environment file you can create and/or update them with:\n$# lxd-compose acl create myproject -u -a Commands #  The commands have different missions:\n  permit to define and register maintenance tasks and/or particolar hooks to run over existing container of already deployed projects. An example could the task that update the lencrypt certificate of existing HTTP service.\n  permit to deploy a specific project with customization (different vars files, flags, etc.). For example, the task to build LXD images over LXD Compose Galaxy is a single project that supply different commands as shortcuts for build the different LXD images.\n  Inside the environment file the commands could be defined inline:\ncommands: - name: mottainai-proxy-update-cerbot description: |Update letencrypt certificate on mottainai Proxy. NOTE: the container must be already created. project: mottainai-server-services apply_alias: true enable_groups: - mottainai-proxy1 enable_flags: - certbot_standalone or through includes:\ninclude_commands_files: - commands/certbot.yml - commands/backup-certbot.yml Obviously, using include_commands_files permit to reuse the same command over multiple projects.\nStorages #  Inside LXD there are different way to setup the LXD storage: btrfs, zfs, lvm, loopback, etc.\nThe storage is the main element when an LXD instance is configured. This is the reason why it\u0026rsquo;s important to trace the configurations option used over a specific remote.\nThe LXD Compose Galaxy has already a good list of possible configurationa that could be used by the users in their projects.\nThe storage specs could be defined inside the environment YAML inline or as included files through the include_storages_files attribute.\nAn example of btrfs loopback storage:\nname: \u0026#34;btrfs-loopback\u0026#34; documentation: | BTRFS Storage Pool Loop disk. driver: \u0026#34;btrfs\u0026#34; config: size: \u0026#34;150GB\u0026#34; btrfs.mount_options: \u0026#34;rw,relatime,space_cache,compress=zstd:3\u0026#34; "}),a.add({id:6,href:'/lxd-compose-docs/docs/galaxy/',title:"Galaxy",section:"LXD Compose",content:"LXD Compose Galaxy #  WIP\n"}),a.add({id:7,href:'/lxd-compose-docs/docs/getting-started/',title:"Getting Started",section:"LXD Compose",content:"Getting Started #  Prerequisites #  lxd-compose doesn\u0026rsquo;t require libraries or tools, just an LXD instance to call.\nGet LXD Compose #  lxd-compose is available as Funtoo Macaroni OS package and installable in every Linux distro through luet tool with these steps:\nInstallation #  $\u0026gt; curl https://raw.githubusercontent.com/geaaru/luet/geaaru/contrib/config/get_luet_root.sh | sh # Install luet on your system $\u0026gt; sudo luet install -y app-emulation/lxd-compose # Install lxd-compose binary $\u0026gt; sudo luet cleanup Upgrade #  $\u0026gt; sudo luet upgrade Create environment tree #  There isn\u0026rsquo;t a specific directory tree required to create an lxd-compose project but hereinafter is explained the best practices to start.\n$\u0026gt; mkdir myproject \u0026amp;\u0026amp; cd myproject $\u0026gt; mkdir -p envs/files # Create directory for static files (optional) $\u0026gt; mkdir -p envs/groups # Create directory for containers groups (optional) $\u0026gt; mkdir -p envs/vars # Create directory for environment variables (optional) The next step is create the lxd-compose configuration file. This file is using YAML format and is automatically read if it\u0026rsquo;s called .lxd-compose.yml.\n$\u0026gt; echo \u0026#34; general: debug: false logging: level: \\\u0026#34;info\\\u0026#34; runtime_cmds_output: true # Define the directories list where load environments. env_dirs: - ./envs \u0026#34; \u0026gt; .lxd-compose.yml Now, it\u0026rsquo;s time to create the project file ./envs/myproject.yml:\nversion: \u0026#34;1\u0026#34; template_engine: engine: \u0026#34;mottainai\u0026#34; projects: - name: \u0026#34;myproject1\u0026#34; description: \u0026#34;My First Project\u0026#34; # A fast way to define environments for template vars: - envs: my_var: \u0026#34;value1\u0026#34; obj: key: \u0026#34;xxx\u0026#34; foo: \u0026#34;baa\u0026#34; groups: - name: \u0026#34;group1\u0026#34; description: \u0026#34;Group 1\u0026#34; # Define the LXD Remote to use and where # create the environment. connection: \u0026#34;local\u0026#34; # Define the list of LXD Profile to use # for create the containers common_profiles: - default # Create the environment container as ephemeral or not. ephemeral: true nodes: - name: node1 image_source: \u0026#34;alpine/3.12\u0026#34; entrypoint: - \u0026#34;/bin/sh\u0026#34; - \u0026#34;-c\u0026#34; # List of commands executed just after the creation of the # container. hooks: - event: post-node-creation commands: - echo \u0026#34;Run container command ${my_var}\u0026#34; # Print node json - event: post-node-creation commands: - apk add curl - curl --no-pregress-meter https://raw.githubusercontent.com/geaaru/luet/geaaru/contrib/config/get_luet_root.sh | sh - luet install utils/jq - echo \u0026#34;${node}\u0026#34; | jq And finally, deploy the project:\n$\u0026gt; lxd-compose apply myproject1 Apply project myproject1 Searching image: alpine/3.12 For image alpine/3.12 found fingerprint 57a2e180cbb8b3daf21a51681232e88d145ebd8955a62ada48f876be86bcd093 Try to download image 57a2e180cbb8b3daf21a51681232e88d145ebd8955a62ada48f876be86bcd093 from remote images... \u0026gt;\u0026gt;\u0026gt; Creating container node1... - üè≠ \u0026gt;\u0026gt;\u0026gt; [node1] - [started] üí£ üè° - echo \u0026#34;Run container command ${my_var}\u0026#34; Run container command value1 \u0026gt;\u0026gt;\u0026gt; [node1] - apk add curl - ‚òï fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.12/community/x86_64/APKINDEX.tar.gz (1/4) Installing ca-certificates (20191127-r4) (2/4) Installing nghttp2-libs (1.41.0-r0) (3/4) Installing libcurl (7.69.1-r3) (4/4) Installing curl (7.69.1-r3) ... The container is been created and all hooks are been executed.\n$\u0026gt; lxc list +--------------------------+---------+----------------------+-----------------------------------------------+-----------------------+-----------+ | NAME | STATE | IPV4 | IPV6 | TYPE | SNAPSHOTS | +--------------------------+---------+----------------------+-----------------------------------------------+-----------------------+-----------+ | node1 | RUNNING | 172.18.10.171 (eth0) | fd42:380a:f674:76f3:216:3eff:fef0:52f8 (eth0) | CONTAINER (EPHEMERAL) | 0 | +--------------------------+---------+----------------------+-----------------------------------------------+-----------------------+-----------+ Congratulations! Your first project has been deployed!\n"}),a.add({id:8,href:'/lxd-compose-docs/docs/concepts/group/',title:"Groups",section:"Concepts",content:"Groups #  The group is used to define for example a list of nodes that supply a particular service. There isn\u0026rsquo;t a real limitation on this but only common/best practices. lxd-compose hasn\u0026rsquo;t limit to use multiple groups with only one node instead of a single group with multiple nodes.\nThe name of the group must be unique inside all loaded environments. This permits to use runtime options to disable/enable groups on deploy a project.  In particular, inside the group you could define:\n  name: an unique identifier of the group. I suggest to avoid spaces.\n  description: an user friendly description of the group mission.\n  connection: the LXD remote/LXD instance to use on manage the LXD containers of the group.\n  common_profiles: an optional attribute to define the list of the LXD profiles to assign at the LXD containers of the group.\n  ephemeral: define if the containers must be created as ephemeral or not.\n  nodes: contains the list of the nodes of the group\n  node_prefix: an optional attribute to define the prefix to use on the name of the nodes. Normally, this is used at runtime to run test units and it\u0026rsquo;s set by the CLI.\n  hooks: an optional attribute to define the list of hooks to apply.\n  config_templates: an optional attribute to define the list of configuration files to generate through the template engine.\n   To show the list of groups available in a project:\n$\u0026gt; # command execute on lxd-compose-galaxy repository $\u0026gt; lxd-compose group list mottainai-server-services - mottainai-database - mottainai-broker - mottainai-server  An example of a group section:\ngroups: - name: \u0026#34;proxy1\u0026#34; description: \u0026#34;Nginx Proxy\u0026#34; connection: \u0026#34;local\u0026#34; # Define the list of LXD Profile to use # for create the containers common_profiles: - default - net-local # Create the environment container as ephemeral or not. ephemeral: false nodes: # ... "}),a.add({id:9,href:'/lxd-compose-docs/docs/concepts/hook/',title:"Hooks",section:"Concepts",content:"Hooks #  The hooks are the operations that could be done to configure the target services.\nInside the hook you could define:\n  event: identify the type of the hook\n  node: an optional attribute that define the name of the container where to run the commands. There is a special node host that is used to run the commands on the host where is running lxd-compose\n  commands: an array of commands executed in the target node\n  out2var: an optional attribute to use when the stdout of the last command must be saved as a variable.\n  err2var: an optional attribute to use when the stderr of the last command must be saved as a variable.\n  entrypoint: an optional attribute to override the entrypoint used to run the commands.\n  flags: an optional list of string that permit to assign a special label to an hook that could be used to exclude the hook or for filter hooks.\n  If a command defined in a hook fails, the chain is interrupted and lxd-compose exiting with error.  Types of events #     Event Type Description     pre-project Hook executed before the deployment phase of the selected project.   pre-group Hook executed before the deployment of a group.   pre-node-creation Hook executed before the creation of a node   post-node-creation Hook executed after the creation of a node   pre-node-sync Hook executed before the sync phase to a node. Also if there aren\u0026rsquo;t files to sync.   post-node-sync Hook executed after the sync phase to a node. Also, if there aren\u0026rsquo;t files to sync.   post-group Hook executed after that all nodes of the group are been creation and/or updated.   post-project Hook executed at the end of the deployment phase of the selected project.   pre-project-shutdown Hook executed before the execution of the destroy actions of the select project.   post-project-shutdown Hook executed after that the project and groups are been destroyed.   pre-group-shutdown Hook executed before the execution of the destroy of the nodes of the group.   post-group-shutdown Hook executed after the execution of the destroy of the nodes of the group.   pre-node-shutdown Hook executed before the execution of the destroy of the node.   post-node-shutdown Hook executed after the execution of the destroy of the node.    The hooks with type pre-node-creation and post-node-creation are executed only if the container is not present and it\u0026rsquo;s created by lxd-compose.\nThe hooks of the type pre-project or post-project can be defined only at project level.\nTo reduce the verbosity of the YAML, lxd-compose permits to define hooks used by multiple groups or node at different levels.\nThe hooks defined to an upper level are merged with the hooks of the bottom level and executed in the defined order for the apply operations:\n   Level Available Event Types (in the execution order)     project pre-project\npre-group\npre-node-creation\npost-node-creation\npre-node-sync\npost-node-sync\npost-group\npost-project   group pre-group\npre-node-creation\npost-node-creation\npre-node-sync\npost-node-sync\npost-group   node pre-node-creation\npost-node-creation\npre-node-sync\npost-node-sync    Hereinafter, the order for the destroy command:\n   Level Available Event Types (in the execution order)     project pre-project-shutdown\npre-group-shutdown\npre-node-shutdown\npost-node-shutdown\npost-group-shutdown\npost-project-shutdown   group pre-group-shutdown\npre-node-shutdown\npost-node-shutdown\npost-group-shutdown   node pre-node-shutdown\npost-node-shutdown    Hooks defined at project level for all nodes #  # Hooks at project level hooks: - event: pre-project # Special node value to execute commands in the host running lxd-compose. # The use of sysctl on host it make sense only for local LXD instances. node: \u0026#34;host\u0026#34; commands: - sysctl -w vm.max_map_count=262144 flags: - host_commands - event: post-node-creation commands: - ACCEPT_LICENSE=\\* equo update \u0026amp;\u0026amp; equo upgrade flags: - update_container This hook is executed after the creation of all nodes of all groups of the project. The node attribute is not needed in this case.\nThe flag update_container permits to exclude the hook on deploy phase.\n# This run the hook on all nodes $\u0026gt; lxd-compose apply myproject # This exclude the hooks with flag update_container $\u0026gt; lxd-compose apply myproject --disable-flag update_container # This execute only the hook with the flag update_container $\u0026gt; lxd-compose apply myproject --enable-flag update_container If it\u0026rsquo;s used the --enable-flag option and the hook doesn\u0026rsquo;t contain flags then it\u0026rsquo;s skipped.\nHooks defined at group level for all nodes or for specific nodes #  In a similar way of the hooks defined at project level the hooks of type pre-node-creation, post-node-creation, pre-node-sync and post-node-sync are applied to all nodes.\nA possible use case could be that to use the post-group event to run infrastructure tests in the last group of the project.\n# List of commands executed just after the creation of the # container. hooks: - event: post-node-creation # Install the python unittest2 package in all nodes. commands: - ACCEPT_LICENSE=\\* equo i dev-python/unittest2 - event: post-group node: \u0026#34;node1\u0026#34; commands: - rm /tmp/foo.log - event: post-group node: \u0026#34;node2\u0026#34; # The run-tests.sh generate traffic/actions to node1. commands: - /root/run-tests.sh - event: post-group node: \u0026#34;node1\u0026#34; commands: - sleep 5 # to manage transmission delays during test - /root/analyze-result.sh The node field could be related also to node not defined in the project. In this case lxd-compose uses the connection of the group where the hook is defined.\nHooks defined at node level #  Also in the hooks defined at node level it\u0026rsquo;s possible to execute hooks on node host, for example, an hook that to store the password automatically generated by the single MySQL instance that could be needed in additional operations and/or for manual tasks.\nHereinafter, an example:\nhooks: - event: post-node-creation commands: - cat /var/log/mysqld.log | grep temporary | awk \u0026#39;{ print $NF }\u0026#39; out2var: \u0026#34;mysql_temporary_pwd\u0026#34; - event: post-node-creation node: \u0026#34;host\u0026#34; commands: - node=$(echo ${node} | jq \u0026#39;.name\u0026#39;) \u0026amp;\u0026amp; echo \u0026#34;${mysql_temporary_pwd}\u0026#34; \u0026gt; ./secrets/${node}.mysql.pwd When lxd-compose runs hooks to host node it initializes the LXD_CONF variable with the same variable of the environment. In this way it is possible to use lxc command directly on hooks.\nhooks: - event: post-node-sync node: \u0026#34;host\u0026#34; entrypoint: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; commands: # Copy files from container to host. - lxc file pull --recursive lxd-instance:nginx1/site envs/files/ "}),a.add({id:10,href:'/lxd-compose-docs/docs/',title:"LXD Compose",section:"LXD Compose",content:"Why LXD Compose? #  While the world trying to move every service over to the cloud at the PAAS level and there are a lot of scenarios that vary in relation to the old school with VMs (Vmware, Openstack, QEMU, etc.).\nIn these scenarios, the use of LXD technology allows for the maintenance of multi-services for containers and applications that are OS or distro specific. (for example: requiring or avoiding SystemD in their scope).\nThe use of lxd-compose with the tool simplestreams-builder for the building of LXD images ensures a method to cleanly reproduce preparing the container\u0026rsquo;s golden images for a production environment. No more worries about rolling updates for the OS used.\nIn the same way, embedded have a way to deploy services in a reproducible manner. For example, to use Banana PI or Raspberry PI at home for Home entertainment, home automation, etc. How many times did you lose your SD card with the configurations?\nIt\u0026rsquo;s here that lxd-compose wants to help people on tracing their configuration and the workflow to follow on setup their infrastructure/services. In particular, also to share this workflow with the community.\nIn the past, I tried to resolve these issues through Ansible with the project FreeRadius Tasks but it\u0026rsquo;s too verbose and few dynamic, it requires dependencies and a lot of RAM.\nSo, in summary, these are the core targets of the lxd-compose:\nSetup LXD instances LXD has so many features and options that are often configured manually after you have installed the instance. These configurations could be applied at the container level (which in this case are lost when you destroy the container) or at the profile level. Personally, I suggest using profiles for this job and indeed lxd-compose tries to share a way to register these profiles used on your configuration service and create these profiles from the specs. The same applies for network devices.  Automation There are a lot of configurations and/or operations to do in a VM or in a container that are repetitive. These steps could be written in a more readable way thanks to the YAML syntax anchors or through Helm engine.  Tracing installation steps How often do you write an Installation Guide of the software written for your clients? To ensure that these steps are always valid and correct in a world where the software changes every minute it\u0026rsquo;s a bit hard. To write a specification that could be to test in a CD/CI pipeline reduce the effort of your team.   Update configurations lxd-compose automatically checks if the container of the project is already present and applies only the hooks related to the configuration. This permits you to update the configuration easily without destroying and creating again the container.  Infrastructure less lxd-compose doesn\u0026rsquo;t require an external service or database to deploy your services. For example like etcd for k8s. You need only a node were to run lxd-compose that reaches one or more LXD instances.  Developers\u0026rsquo;s friend Often it\u0026rsquo;s not easy for a developer to test code when other developers are, at the same time, working on modules of the same infrastructure. lxd-compose works to supply a method to quickly, reliably, and repetitively deploy an infrastructure for testing and for syncing code directly in the container used by the developer for testing code.   "}),a.add({id:11,href:'/lxd-compose-docs/docs/concepts/node/',title:"Nodes",section:"Concepts",content:"Nodes #  The node identify the container created to a specific LXD instance defined by the connection attribute available at group level.\nThe name of the node must be unique inside all loaded environments. In the real the limitation is mandatory only for the same LXD instance, but lxd-compose validate at the moment doesn\u0026rsquo;t permit to use the same name between different groups or projects.  In particular, inside the node you could define:\n  name: the name of the container used inside the LXD container instance\n  name_prefix: an optional attribute to define the prefix to use on the name of the node. Normally, this is used at runtime to run test unites and it\u0026rsquo;s set by the CLI.\n  image_source: the name of the image to use for create the container.\n  image_remote_server: an optional attribute to define the name of the remote where to search the used source image. By default it\u0026rsquo;s used Canonical images server if P2P mode is disabled.\n  labels: an optional attribute to define key values field of the specific node that are then set as environment variables of the hooks executed for the node.\n  source_dir: an optional attribute to define the source directory to use in the join with the configuration files to use with the template engine or for the files/directories to sync.\n  entrypoint: an optional attribute to define the entrypoint to use on run command inside the container. By default it\u0026rsquo;s used /bin/bash -c.\n  hooks: an optional attribute to define the list of hooks to apply.\n  config_templates: an optional attribute to define the list of configuration files to generate through the template engine.\n  sync_resources: an optional attribute to define the list of files or directories to sync from the host where is running lxd-compose to the container.\n  An example of a node section:\nnodes: - \u0026amp;mongors1 name: mongo-rs1 image_source: \u0026#34;ubuntu/18.04\u0026#34; # By deafult it use remote images\u0026#34; image_remote_server: \u0026#34;images\u0026#34; entrypoint: - \u0026#34;/bin/bash\u0026#34; - \u0026#34;-c\u0026#34; # Define the list of LXD Profile to use in additional # to group profiles for create the containers #profiles: # - privileged # List of commands executed just after the creation of the # container. hooks: - event: post-node-creation commands: # DHCP seems slow - sleep 5 - apt-get update - apt-get upgrade -y - apt-get install wget gpg ca-certificates jq -y - |wget -q -O /usr/bin/yq \\ https://github.com/mikefarah/yq/releases/download/3.4.1/yq_linux_amd64 - chmod a+x /usr/bin/yq - apt-get clean - event: post-node-creation commands: - apt-get install -y $(echo ${packages} | jq \u0026#39;.[]\u0026#39; -r) - apt-get clean sync_resources: # The path files is under the directory of the # environment file. - source: files/mongo-setup.sh dst: /tmp/mongo-setup.sh - \u0026lt;\u0026lt;: *mongors1 name: mongo-rs2 - \u0026lt;\u0026lt;: *mongors1 name: mongo-rs3 On setup multiple nodes of the same group with the same configuration options you can to use the YAML ancors and to reduce the specification.  "}),a.add({id:12,href:'/lxd-compose-docs/docs/packaging/',title:"Packaging Projects",section:"LXD Compose",content:"Packaging Projects #  In order to organize your projects for a Production environment it\u0026rsquo;s possible that the team that executes the installation is not the same one that has prepared the specs. Again, the production environment is for security reasons without a direct connection with the SCM repositories where are available the templates to use for the configuration file generation.\nAnother example is that the templates of the configuration the file of the modules implemented is strictly related to a specific version and the lxd-compose specs must follow this relationship for use.\nIt\u0026rsquo;s here that the new command pack and unpack try to help the workflow operative that permits to have a way to share independent the lxd-compose specifications to a Production Engineer Team and follow the installation and upgrade process when the separation of duties doesn\u0026rsquo;t permit to have access directly to source repositories.\nIn particular, the pack command permits to the creation of a tarball of the lxd-compose specs and all the files and templates used for the system setup. It considers some best practices to follow that simplify the paths remapping of the sources used.\nHereinafter, an easy example that describes a possible use case.\nExample #  In the example we consider to have two different git repositories for the lxd-compose specs and the developed module where it\u0026rsquo;s also available the template of his configuration file:\nIn the example, we consider having two different git repositories for the lxd-compose specs and the developed module where it\u0026rsquo;s also available the template of his configuration file.\n  my-lxd-compose: the repository of the LXD Compose specs.\n  my-module: the repository of the developed module.\n  The two repositories are organized in this way:\n $# tree . ‚îú‚îÄ‚îÄ lxd-compose ‚îÇ¬†‚îú‚îÄ‚îÄ envs ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ common ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ hooks ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ hosts.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ luet-packages.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ luet-repositories.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ node-exporter-systemd.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ node-exporter-sysvinit.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ systemd-dns.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ systemd-net-static.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ systemd-net.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ ubuntu-setup.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ yum-setup.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ networks ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ mottainai0.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ ovs0.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ profiles ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ default.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ docker-xfs-fs.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ docker.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ flavor-big.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ flavor-medium.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ flavor-thin.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ logs-disk.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ loop.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ lxd-socket-proxy.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ lxd-socket.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ lxd-vm.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ net-mottainai0.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ net-phy-mgmt.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ net-phy-srv.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ privileged.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ zfs.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ storages ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ btrfs-loopback.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ btrfs-source.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ ceph.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ dir-source.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ lvm-loopback.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ lvm-source.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ zfs-loopback.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ zfs-source.yml ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ myproject ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ my.yml ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ vars ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ common.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ lxd-conf ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ client.crt ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ client.key ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ config.yml ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ servercerts ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ 127.0.0.1.crt ‚îÇ¬†‚îî‚îÄ‚îÄ render ‚îÇ¬†‚îî‚îÄ‚îÄ default.yaml ‚îî‚îÄ‚îÄ my-module ‚îú‚îÄ‚îÄ conf.tmpl ‚îî‚îÄ‚îÄ mymodule.sh 13 directories, 45 files This the lxd-compose config file lxd-compose/.lxd-compose.yml:\ngeneral: lxd_confdir: ./lxd-conf logging: enable_logfile: false level: info enable_emoji: true color: true runtime_cmds_output: true cmds_output: true env_dirs: - envs/myproject/ render_default_file: render/default.yaml Between the different best practices a good choice when the templates are available in different repositories is to use a render environment as a prefix path. The content of this variable will be used for the pack renaming later.\nThis the content of the file render/default.yaml:\n#-------------------------------------------# # General params #-------------------------------------------# connection: \u0026#34;local\u0026#34; ephemeral: true default_ubuntu_image: \u0026#34;ubuntu/22.04\u0026#34; default_ubuntu_lts_image: \u0026#34;ubuntu/18.04\u0026#34; default_internal_domain: \u0026#34;mottainai.local\u0026#34; source_base_dir: \u0026#34;../../..\u0026#34; The source_base_dir is the render env used in the project for the compilation of the module file.\nObviously, it\u0026rsquo;s better to use the same tree level for all projects and environments to improve readability.\nSo, if we have a very simple module/bashing script that just has a source file like this:\n$\u0026gt; cat my-module/conf.tmpl #!/bin/bash export msg=\u0026#34;{{ .message }}\u0026#34; That is imported in the main script file:\n$ cat my-module/mymodule.sh #!/bin/bash # The conf.sh file is generated by lxd-compose from conf.tmpl. source /etc/conf.sh for ((i=0; i\u0026lt;5;i++)); do echo $msg done The message variable is defined in the var file:\n$\u0026gt; cat lxd-compose/envs/myproject/vars/common.yaml envs: message: \u0026#34;W LXD Compose\u0026#34; In particular, the file conf.tmpl is the file compiled by lxd-compose to generate the conf.sh file imported by the script mymodule.sh. The both files are then synced in the container like described in this environment specs:\n$ cat lxd-compose/envs/myproject/my.yml # Author: Daniele Rondina, geaaru@funtoo.org version: \u0026#34;1\u0026#34; template_engine: engine: \u0026#34;mottainai\u0026#34; include_profiles_files: - ../common/profiles/net-mottainai0.yml - ../common/profiles/default.yml - ../common/profiles/flavor-medium.yml - ../common/profiles/loop.yml - ../common/profiles/docker.yml - ../common/profiles/privileged.yml include_networks_files: - ../common/networks/mottainai0.yml - ../common/networks/ovs0.yml include_storage_files: - ../common/storages/dir-source.yml - ../common/storages/btrfs-source.yml pack_extra: files: - {{ .Values.source_base_dir }}/my-module/mymodule.sh rename: # The packth is related to the lxd-compose directory and the # environment file basedir. - source: ../my-module/mymodule.sh dest: sources/my-module/mymodule.sh projects: - name: \u0026#34;myproject\u0026#34; description: | Testing project for pack command. include_env_files: - vars/common.yaml vars: - envs: LUET_NOLOCK: \u0026#34;true\u0026#34; LUET_YES: \u0026#34;true\u0026#34; luet_packages: - net-tools - bash groups: - name: \u0026#34;my-module-service\u0026#34; description: \u0026#34;Start container for running my-module.\u0026#34; include_hooks_files: - ../common/hooks/luet-packages.yml common_profiles: - default - net-mottainai0 # Create the environment container as ephemeral or not. ephemeral: true connection: \u0026#34;{{ .Values.connection }}\u0026#34; nodes: - name: mymodule1 image_source: \u0026#34;macaroni/terragon-dumplings\u0026#34; image_remote_server: \u0026#34;macaroni\u0026#34; hooks: - event: post-node-sync commands: # Running mymodule - bash /mymodule.sh config_templates: - source: {{ .Values.source_base_dir }}/my-module/conf.tmpl dst: /tmp/lxd-compose/mymodule1/etc/conf.sh sync_resources: - source: {{ .Values.source_base_dir }}/my-module/mymodule.sh dst: / - source: /tmp/lxd-compose/mymodule1/etc/conf.sh dst: /etc/ As visible in the specs the source_base_dir render variable is used as a prefix path in the config_templates and sync_resources sections.\nThe pack command just includes automatically all files defined in the config_templates, profiles, groups files, variable files, networks, and commands but NOT the additional files added in the sync_resources section. This is because often it contains files generated from templates. If there is a sync of an extra file this file must be defined in the pack_extra section to be included in the tarball. This means that when there is a static file to inject in the tarball this path must be renamed to sources directory created automatically. If this files are stored under the lxd-compose project the rename is not neeeded.  When the project is ready for Production we can generate the tarball with this command:\n$\u0026gt; lxd-compose pack --source-common-path \u0026#34;../../..\u0026#34; --to /tmp/lxd-compose-myproject.tar.gz myproject üè≠ Processing project myproject with env file envs/myproject/my.yml. Template ../my-module/conf.tmpl -\u0026gt; sources/my-module/conf.tmpl Tarball /tmp/lxd-compose-myproject.tar.gz generated. The source dir to use is: ../../sources The option --source-common-path normally is set with the same value defined in the render env for the variable source_base_dir.\nThe tarball /tmp/lxd-compose-project.tar.gz is ready for the installation!\nSo, in a fresh filesystem it\u0026rsquo;s possible unpack the tarball with this command:\n$\u0026gt; lxd-compose unpack --render-file render/default.yaml --render-env \u0026#34;source_base_dir=../../sources\u0026#34; /tmp/lxd-compose-myproject.tar.gz Render file render/default.yaml updated correctly. Operation completed. That for the example generates this tree:\n\u0026lt;lxd-compose-demo\u0026gt;# tree . ‚îú‚îÄ‚îÄ envs ‚îÇ¬†‚îú‚îÄ‚îÄ common ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ hooks ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ luet-packages.yml ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ networks ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ mottainai0.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ ovs0.yml ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ profiles ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ default.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ docker.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ flavor-medium.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ loop.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ net-mottainai0.yml ‚îÇ¬†‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ privileged.yml ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ storages ‚îÇ¬†‚îÇ¬†‚îú‚îÄ‚îÄ btrfs-source.yml ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ dir-source.yml ‚îÇ¬†‚îî‚îÄ‚îÄ myproject ‚îÇ¬†‚îú‚îÄ‚îÄ my.yml ‚îÇ¬†‚îî‚îÄ‚îÄ vars ‚îÇ¬†‚îî‚îÄ‚îÄ common.yaml ‚îú‚îÄ‚îÄ lxd-conf ‚îÇ¬†‚îú‚îÄ‚îÄ client.crt ‚îÇ¬†‚îú‚îÄ‚îÄ client.key ‚îÇ¬†‚îú‚îÄ‚îÄ config.yml ‚îÇ¬†‚îî‚îÄ‚îÄ servercerts ‚îÇ¬†‚îî‚îÄ‚îÄ 127.0.0.1.crt ‚îú‚îÄ‚îÄ render ‚îÇ¬†‚îî‚îÄ‚îÄ default.yaml ‚îî‚îÄ‚îÄ sources ‚îî‚îÄ‚îÄ my-module ‚îú‚îÄ‚îÄ conf.tmpl ‚îî‚îÄ‚îÄ mymodule.sh 13 directories, 20 files As is visible, in the tarball files are automatically injected all files used by the selected project like for example the hook file envs/common/luet-packages.yml but not the file envs/common/luet-repositories.yml.\nAt the same time, the pack command injects the lxd-conf directory if it\u0026rsquo;s been defined in the lxd-compose config file.\nAfter, the unpack the tree is ready for the deploy:\n$\u0026gt; lxd-compose a myproject --env sleep=3 Apply project myproject Searching image: macaroni/terragon-dumplings For image macaroni/terragon-dumplings found fingerprint 109aec564696c4757cf0036eca5311c6f447dbf9c2ef4afc2e8db43f7c8fe98b \u0026gt;\u0026gt;\u0026gt; Creating container mymodule1... - üè≠ \u0026gt;\u0026gt;\u0026gt; [mymodule1] - [started] üí£ \u0026gt;\u0026gt;\u0026gt; [mymodule1] - [ -n \u0026#34;${sleep}\u0026#34; ] \u0026amp;\u0026amp; sleep ${sleep} ; if [ -e /etc/os-release ] ; then ubuntu=$(cat /etc/os-release | grep ID| grep ubuntu | wc -l) ; else ubuntu=\u0026#34;0\u0026#34; ; fi \u0026amp;\u0026amp; luet repo update \u0026amp;\u0026amp; luet i -y utils/jq utils/yq \u0026amp;\u0026amp; luet i -y $(echo ${luet_packages} | jq \u0026#39;.[]\u0026#39; -r) \u0026amp;\u0026amp; luet cleanup --purge-repos ; - ‚òï üè† Repository: geaaru-repo-index Revision: 3 - 2023-02-07 14:36:23 +0000 UTC üè† Repository: mottainai-stable Revision: 67 - 2023-02-20 18:13:58 +0000 UTC üè† Repository: macaroni-commons Revision: 117 - 2023-01-08 09:28:23 +0000 UTC üè† Repository: macaroni-terragon Revision: 143 - 2023-02-07 09:11:27 +0000 UTC ... Resolve finalizers... üöÄ Luet 0.33.0-geaaru-g4e8db62fb8d2b25df7652f5001353fcde8893197 2023-02-19 07:42:11 UTC - go1.20.1 üè† Repository: geaaru-repo-index Revision: 3 - 2023-02-07 14:36:23 +0000 UTC üè† Repository: macaroni-commons Revision: 117 - 2023-01-08 09:28:23 +0000 UTC üè† Repository: macaroni-terragon Revision: 143 - 2023-02-07 09:11:27 +0000 UTC üè† Repository: mottainai-stable Revision: 67 - 2023-02-20 18:13:58 +0000 UTC üöß warning sys-apps/net-tools already installed. üöß warning app-shells/bash already installed. üöß warning No packages to install. Cleaned: 17 packages. Repos Cleaned: 4 \u0026gt;\u0026gt;\u0026gt; [mymodule1] Compile 1 resources... üç¶ \u0026gt;\u0026gt;\u0026gt; [mymodule1] - [ 1/ 1] /tmp/lxd-compose/mymodule1/etc/conf.sh ‚úî \u0026gt;\u0026gt;\u0026gt; [mymodule1] Syncing 2 resources... - üöå \u0026gt;\u0026gt;\u0026gt; [mymodule1] - [ 1/ 2] / - ‚úî \u0026gt;\u0026gt;\u0026gt; [mymodule1] - [ 2/ 2] /etc/ - ‚úî \u0026gt;\u0026gt;\u0026gt; [mymodule1] - bash /mymodule.sh - ‚òï W LXD Compose W LXD Compose W LXD Compose W LXD Compose W LXD Compose All done. "}),a.add({id:13,href:'/lxd-compose-docs/docs/concepts/project/',title:"Projects",section:"Concepts",content:"Projects #  The project is used to define for example a way to identify the deploy of one or more services (inside different groups).\nThe name of the project must be unique inside all loaded environments.  In particular, inside the project you could define:\n  name: an unique identifier of the project. I suggest to avoid spaces.\n  description: an user friendly description of the project mission.\n  include_env_files: an optional attribute that permit to define the list of files with environment variables loaded by lxd-compose. If the path is relative this is based on the directory where is present the environment file that contains the project.\n  include_groups_files: an optional attribute that permit to define the list of files where are defined the groups of the project. If the path is relative this is based on the directory where is present the environment file that contains the project.\n  include_hooks_files: an optional attribute that permit to define the list of files where are defined the hooks of the project. If the path is relative this is based on the directory where is present the environment file that contains the project.\n  vars: an optional attribute to define inline project variables.\n  groups: an optional attribute to define inline groups of the project.\n  node_prefix: an optional attribute to define the prefix to use on the name of the nodes. Normally, this is used at runtime to run test units and it\u0026rsquo;s set by the CLI.\n  hooks: an optional attribute to define the list of hooks to apply.\n  config_templates: an optional attribute to define the list of configuration files to generate through the template engine.\n   To show the list of all available projects:\n$\u0026gt; # command execute on lxd-compose-galaxy repository $\u0026gt; lxd-compose project list - sonarqube-ce - nginx-proxy - docker-registry-services - mongo-replica-set - mottainai-server-services - luet-runner-amd64 - arm::ubuntu::mottainai-agent  An example of a project section:\nprojects: - name: \u0026#34;nginx-proxy\u0026#34; description: |Setup NGINX proxy with custom reverse balancer, integrated with letencrypt. # Include variables files include_env_files: - vars/main.yml # Include groups files include_groups_files: - groups/grp1.yml # Inline variables vars: - envs: mypublic_domain: example1.com letencrypt_server: https://acme-v02.api.letsencrypt.org/directory letencrypt_email: myemail@example.com # Inline groups groups: - name: \u0026#34;proxy1\u0026#34; description: \u0026#34;Nginx Proxy\u0026#34; # In this case it\u0026#39;s used local remote. connection: \u0026#34;local\u0026#34; # Define the list of LXD Profile to use # for create the containers common_profiles: - default - net-local # Create the environment container as ephemeral or not. ephemeral: false nodes: # ... "}),a.add({id:14,href:'/lxd-compose-docs/docs/rpi4/',title:"Raspberry Pi4",section:"LXD Compose",content:"LXD Compose on Raspberry Pi4 #  WIP\n"}),a.add({id:15,href:'/lxd-compose-docs/docs/render_engine/',title:"Render Engine",section:"LXD Compose",content:"Render Engine #  The term render engine is used in lxd-compose to identify the engine that is usable to convert lxd-compose specifications where are used template syntax. This conversion is done at runtime directly in loading phase by lxd-compose when is defined a render_default_file or render_values_file.\n"}),a.add({id:16,href:'/lxd-compose-docs/docs/template_engine/',title:"Template Engine",section:"LXD Compose",content:"Template Engine #  The term template engine is used in lxd-compose to identify the engine that is used to generate projects\u0026rsquo;s files used in the deploy process but not related to the lxd-compose specification.\nIn the deploy process could be possible that we need to generate configuration files based on the project variables.\nIn lxd-compose there are two way to use the template engine:\n  jinja2: through the tool j2cli it\u0026rsquo;s possible to generate configuration files based on Jinja2 template engine.\n  mottainai: this doesn\u0026rsquo;t require external tool from the system. It use golang template engine with additiona macro from the Mottainai project and from sprig.\n  The generation of the files from the template engine is done from the config_templates option available at project level, group level or node level.\nIf there aren\u0026rsquo;t variables defined at runtime through the hooks it\u0026rsquo;s possible testing compilation of the templates without container with the command:\n$\u0026gt; lxd-compose compile -p nginx-proxy \u0026gt;\u0026gt;\u0026gt; [nginx1] Compile 1 resources... üç¶ \u0026gt;\u0026gt;\u0026gt; [nginx1] - [ 1/ 1] /tmp/nginx/nginx.conf ‚úî Compilation completed! If you have variables generated through the hooks it\u0026rsquo;s possible to use the lxd-compose compile commands but the missing variables will be empty.\nJinja2 Engine #  The jinja2 engine is the same engine used by Ansible. It requires the j2cli tool installed.\nMore details about jinja engine are available in the project site.\nThe engine is defined at environment level in this way:\ntemplate_engine: engine: \u0026#34;jinja2\u0026#34; # For jinja2 there are a lot filter in ansible package # that could be loaded with: opts: # Enable to_yaml, to_json, etc. - \u0026#34;--filters\u0026#34; - \u0026#34;/usr/lib/python3.7/site-packages/ansible/plugins/filter/core.py\u0026#34; - \u0026#34;contrib/filters/from_json.py\u0026#34; How described in the j2cli documentation it\u0026rsquo;s possible to add additional plugin to extend the available macro to use in your template files.\nIn the example, they are used the Ansible macros and a custom filter.\nWhen the engine configuration is done you can define the files to generate inside the config_templates section in this way:\n# List of templates files to compiles before push the # result inside container. config_templates: - source: files/template.j2 dst: files/myconf.yaml where files/template.j2 is the template file and the files/myconf.yaml is the generated file that could be used later with the sync_resources to push the file directly in the container.\nThe template file (in this case template.j2) could be something like this:\nnode: # This creates node information of lxd-compose specification in YAML format. {{ node | to_nice_yaml(indent=2) | indent(2, true) }} project: # This creates project information of lxd-compose specification in YAML format. {{ project | to_nice_yaml(indent=2) | indent(2, true) }} # key1 and key2 are defined as variable of the project or generated by hooks. key1: {{ key1 }} key2: {{ key2 }} # key_from_file1 is a variable defined inside the variable file. key_from_file: {{ key_from_file1 }} {{ json_var | from_json | to_nice_yaml(indent=2) | indent(2, true) }} {% for user in json_var | from_json %} {{ user.user }} {% endfor %} This an example of one of the included variables files:\nenvs: key_from_file1: \u0026#34;xx\u0026#34; There isn\u0026rsquo;t a limitation about the types of files to generate. You can generate YAML file, JSON file, etc.\nGenerated output $ cat contrib/examples/envs/files/myconf.conf.yaml  node: config_templates: - dst: files/myconf.conf.yaml source: files/template.j2 entrypoint: - /bin/sh - -c hooks: - commands: - echo \u0026#34;Run host command\u0026#34; event: post-node-creation node: host - commands: - echo \u0026#34;1\u0026#34; event: post-node-creation node: \u0026#39;\u0026#39; - commands: - apk add curl - curl --no-pregress-meter https://raw.githubusercontent.com/geaaru/luet/geaaru/contrib/config/get_luet_root.sh | sh - luet install utils/jq - echo \u0026#34;${node}\u0026#34; | jq event: post-node-creation node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;HOST PRE-NODE-SYNC\u0026#34; event: pre-node-sync node: host - commands: - echo \u0026#34;Start app\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; out2var: myvar - commands: - echo \u0026#34;${myvar}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${key1}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${obj}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${mynode_data1}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;HOST ${myvar}\u0026#34; - echo \u0026#34;${myvar}\u0026#34; event: post-node-sync node: host - commands: - echo \u0026#34;${myvar}\u0026#34; \u0026gt; /tmp/lxd-compose-var entrypoint: - /bin/bash - -c event: post-node-sync flags: - flag1 node: host - commands: - \u0026#39;echo \u0026#39;\u0026#39;{ \u0026#34;obj1\u0026#34;: \u0026#34;value1\u0026#34; }\u0026#39;\u0026#39; | jq \u0026#39;\u0026#39;.obj1\u0026#39;\u0026#39; \u0026#39; entrypoint: - /bin/bash - -c event: post-node-sync flags: - flag2 node: host out2var: host_var - commands: - echo \u0026#34;${host_var}\u0026#34; entrypoint: - /bin/bash - -c event: post-node-sync flags: - flag2 node: host - commands: - echo \u0026#34;${json_var}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${runtime_var}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; image_source: alpine/3.12 labels: mynode_data1: data1 name: node1 sync_resources: - dst: /etc/myapp/myconf.conf.yaml source: files/myconf.conf.yaml - dst: /etc/myapp2/ source: files/ project: description: LXD Compose Example1 groups: - common_profiles: - default - net-mottainai0 connection: local description: Description1 ephemeral: true hooks: - commands: - echo \u0026#34;HOST PRE-NODE-SYNC (ON GROUP)\u0026#34; event: pre-node-sync node: host name: group1 nodes: - config_templates: - dst: files/myconf.conf.yaml source: files/template.j2 entrypoint: - /bin/sh - -c hooks: - commands: - echo \u0026#34;Run host command\u0026#34; event: post-node-creation node: host - commands: - echo \u0026#34;1\u0026#34; event: post-node-creation node: \u0026#39;\u0026#39; - commands: - apk add curl - curl https://raw.githubusercontent.com/geaaru/luet/geaaru/contrib/config/get_luet_root.sh | sh - luet install utils/jq - echo \u0026#34;${node}\u0026#34; | jq event: post-node-creation node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;HOST PRE-NODE-SYNC\u0026#34; event: pre-node-sync node: host - commands: - echo \u0026#34;Start app\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; out2var: myvar - commands: - echo \u0026#34;${myvar}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${key1}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${obj}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${mynode_data1}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;HOST ${myvar}\u0026#34; - echo \u0026#34;${myvar}\u0026#34; event: post-node-sync node: host - commands: - echo \u0026#34;${myvar}\u0026#34; \u0026gt; /tmp/lxd-compose-var entrypoint: - /bin/bash - -c event: post-node-sync flags: - flag1 node: host - commands: - \u0026#39;echo \u0026#39;\u0026#39;{ \u0026#34;obj1\u0026#34;: \u0026#34;value1\u0026#34; }\u0026#39;\u0026#39; | jq \u0026#39;\u0026#39;.obj1\u0026#39;\u0026#39; \u0026#39; entrypoint: - /bin/bash - -c event: post-node-sync flags: - flag2 node: host out2var: host_var - commands: - echo \u0026#34;${host_var}\u0026#34; entrypoint: - /bin/bash - -c event: post-node-sync flags: - flag2 node: host - commands: - echo \u0026#34;${json_var}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; - commands: - echo \u0026#34;${runtime_var}\u0026#34; event: post-node-sync node: \u0026#39;\u0026#39; image_source: alpine/3.12 labels: mynode_data1: data1 name: node1 sync_resources: - dst: /etc/myapp/myconf.conf.yaml source: files/myconf.conf.yaml - dst: /etc/myapp2/ source: files/ hooks: - commands: - \u0026#39;echo \u0026#39;\u0026#39;[{ \u0026#34;user\u0026#34;: \u0026#34;user1\u0026#34; }]\u0026#39;\u0026#39; \u0026#39; event: pre-group node: host out2var: json_var - commands: - \u0026#39;echo \u0026#39;\u0026#39;[{ \u0026#34;user\u0026#34;: \u0026#34;user1\u0026#34; }]\u0026#39;\u0026#39; \u0026#39; event: pre-group node: host out2var: json_var include_env_files: - ../vars/file1.yml name: lxd-compose-example1 vars: - envs: LUET_YES: \u0026#39;true\u0026#39; key1: value1 key2: value2 obj: foo: baa key: xxx - envs: json_var: \u0026#39;[{ \u0026#34;user\u0026#34;: \u0026#34;user1\u0026#34; }] \u0026#39; key_from_file1: xx key1: value1 key2: value2 key_from_file: xx - user: user1 user1    Mottainai Engine #  In a similar way, the mottainai engine could be used to generate files used in the deploy workflow.\nIt\u0026rsquo;s used the Golang template engine with additional functions from Mottainai Server project and from sprig.\nThis engine doesn\u0026rsquo;t require external tools.\nHereinafter, an example related to an NGINX configuration:\nnginx.conf.tmpl user {{ .nginx_user }}; worker_processes auto; pid /run/nginx.pid; include /etc/nginx/modules-enabled/*.conf; events { worker_connections 1024; use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#39; \u0026#39;\u0026#34;$request\u0026#34; $status $bytes_sent \u0026#39; \u0026#39;\u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; \u0026#39; \u0026#39;\u0026#34;$gzip_ratio\u0026#34;\u0026#39;; client_header_timeout 10m; client_body_timeout 10m; send_timeout 10m; connection_pool_size 256; client_header_buffer_size 1k; large_client_header_buffers 4 2k; request_pool_size 4k; gzip off; output_buffers 1 32k; postpone_output 1460; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 75 20; ignore_invalid_headers on; index index.html; proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off; {{ range $index, $upstream := .nginx_upstreams }} upstream {{ index $upstream \u0026#34;name\u0026#34; }} { server {{ index $upstream \u0026#34;server\u0026#34; }}; keepalive {{ index $upstream \u0026#34;keepalive\u0026#34; }}; } {{ end }} server { listen 80; server_name {{ .mypublic_domain }}; server_tokens off; access_log /var/log/nginx/access_log main; error_log /var/log/nginx/error_log info; {{ range $index, $loc := .nginx_location_http }} location {{ index $loc \u0026#34;path\u0026#34; }} { {{ index $loc \u0026#34;content\u0026#34; }} } {{ end }} } server { listen 443 ssl; server_name {{ .mypublic_domain }}; server_tokens off; ssl_certificate /certbot/live/{{ .mypublic_domain }}/fullchain.pem; #ssl_certificate /certbot/live/{{ .mypublic_domain }}/cert.pem; ssl_certificate_key /certbot/live/{{ .mypublic_domain }}/privkey.pem; access_log /var/log/nginx/ssl_access_log main; error_log /var/log/nginx/ssl_error_log info; {{ range $index, $loc := .nginx_location_ssl }} location {{ index $loc \u0026#34;path\u0026#34; }} { {{ index $loc \u0026#34;content\u0026#34; | nindent 10 }} } {{ end }} root /var/www/html; } } If we consider a variable file like this:\nenvs: nginx_user: www-data nginx_logrotate_days: 30 nginx_upstreams: - name: upstream1 server: 192.168.0.90:8065 keepalive: 32 nginx_reset_htpasswd: \u0026#34;1\u0026#34; nginx_auth_basic_files: - path: /etc/nginx/myauth users: - user: \u0026#34;user1\u0026#34; pwd: \u0026#34;xxxxxx\u0026#34; - user: \u0026#34;user2\u0026#34; pwd: \u0026#34;yyyyy\u0026#34; nginx_location_http: - path: \u0026#34;/\u0026#34; content: | deny all; nginx_location_ssl: - path: \u0026#34;/\u0026#34; content: | deny all; - path: \u0026#34;/public/\u0026#34; content: | allow all; - path: \u0026#34;/private/\u0026#34; content: |satisfy all; #allow 192.168.0.0/24; #deny all; index index.htm; auth_basic \u0026#34;Restricted Area\u0026#34;; auth_basic_user_file /etc/nginx/myauth;    the output generated is something like available below:\nGenerated output user www-data; worker_processes auto; pid /run/nginx.pid; include /etc/nginx/modules-enabled/*.conf; events { worker_connections 1024; use epoll; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#39; \u0026#39;\u0026#34;$request\u0026#34; $status $bytes_sent \u0026#39; \u0026#39;\u0026#34;$http_referer\u0026#34; \u0026#34;$http_user_agent\u0026#34; \u0026#39; \u0026#39;\u0026#34;$gzip_ratio\u0026#34;\u0026#39;; client_header_timeout 10m; client_body_timeout 10m; send_timeout 10m; connection_pool_size 256; client_header_buffer_size 1k; large_client_header_buffers 4 2k; request_pool_size 4k; gzip off; output_buffers 1 32k; postpone_output 1460; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 75 20; ignore_invalid_headers on; index index.html; proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=mattermost_cache:10m max_size=3g inactive=120m use_temp_path=off; upstream upstream1 { server 192.168.0.90:8065; keepalive 32; } server { listen 80; server_name example1.com; server_tokens off; access_log /var/log/nginx/access_log main; error_log /var/log/nginx/error_log info; location / { deny all; } } server { listen 443 ssl; server_name example1.com; server_tokens off; ssl_certificate /certbot/live/example1.com/fullchain.pem; #ssl_certificate /certbot/live/example1.com/cert.pem; ssl_certificate_key /certbot/live/example1.com/privkey.pem; access_log /var/log/nginx/ssl_access_log main; error_log /var/log/nginx/ssl_error_log info; location / { deny all; } location /public/ { allow all; } location /private/ { satisfy all; #allow 192.168.0.0/24; #deny all; index index.htm; auth_basic \u0026#34;Restricted Area\u0026#34;; auth_basic_user_file /etc/nginx/myauth; } root /var/www/html; } }    "}),a.add({id:17,href:'/lxd-compose-docs/docs/concepts/variable/',title:"Variables",section:"Concepts",content:"Variables #  One of the more important feature of lxd-compose is the management of the variables.\nA variable could be identified by a simple key and a value that could be a simple string/integer or a complex struct.\nEvery key will be available inside the execution of the hooks as an environemnt variable, in the case of simple type will be available As-is, instead for complex struct lxd-compose will convert it in JSON string to parse for example with jq tool.\nBefore to describe how the variables are used inside the hooks i will describe the different way to set a specific variable inside lxd-compose.\nFilter #  As described in the issue #16 using variables too big or complex could generates some issues in LXD.\nSo, it\u0026rsquo;s possible define at project level the list of the variables to exclude from automatic injection in the execute shell of the commands.\nshell_envs_filter: - filteredEnv The filtered variables will be available only for the template engine.\nInline variable #  Under a project it\u0026rsquo;s possible define an inline variable in different envs object:\n# Inline variable vars: - envs: mypublic_domain: example1.com letencrypt_server: https://acme-v02.api.letsencrypt.org/directory letencrypt_email: myemail@example.com Variable\u0026rsquo;s file #  From a variables\u0026rsquo;s file inject by the include_env_files attribute.\nenvs: nginx_user: www-data nginx_logrotate_days: 30 nginx_upstreams: - name: upstream1 server: 192.168.0.90:8065 keepalive: 32 nginx_reset_htpasswd: \u0026#34;1\u0026#34; nginx_auth_basic_files: - path: /etc/nginx/myauth users: - user: \u0026#34;user1\u0026#34; pwd: \u0026#34;xxxxxx\u0026#34; - user: \u0026#34;user2\u0026#34; pwd: \u0026#34;yyyyy\u0026#34; Node\u0026rsquo;s variables from labels #  From the node labels:\nlabels: label1: \u0026#34;1\u0026#34; The node labels help to setup single node variables and when defined these options are also configured as container user.* configuration params. Normally, this variables could be used over LXD template engine integrated inside the LXD image and so a label with name myvar will be added inside the container configuration option as user.myvar.\nRuntime variables #  It\u0026rsquo;s possible set variable on deploy a project from CLI:\n$\u0026gt; lxd-compose apply myproject --env \u0026#39;key1=value\u0026#39; --env \u0026#39;key2=value\u0026#39; Runtime inject variable file #  It\u0026rsquo;s possible inject a variable\u0026rsquo;s file on deploy a project from CLI:\n$\u0026gt; lxd-compose apply myproject --vars-file vars.yml Set a variable at runtime on the execution of the hooks #  An hooks could be used to set a variable from the stdout or stderr generated by the last command of the hook.\n- event: pre-group node: \u0026#34;host\u0026#34; commands: - echo \u0026#39;[{ \u0026#34;user\u0026#34;: \u0026#34;user1\u0026#34; }]\u0026#39; out2var: \u0026#34;json_var\u0026#34; It\u0026rsquo;s used out2var for stdout and err2var for stderr.\n These modes to configure variables inside a project are used to inizialize a map in memory that is then used from the template engine to render configuration files and as environment variables of the shell commands executed.\nFor example inside an hook the variable:\nenvs: nginx_user: www-data could be used in this way:\n- event: post-node-creation commands: - chown ${nginx_user}:${nginx_user} /mywww -R Instead a complex variable like this:\nenvs: obj: key: \u0026#34;xxx\u0026#34; foo: \u0026#34;baa\u0026#34; will be available inside the command in this way:\n\u0026gt;\u0026gt;\u0026gt; [node1] - echo \u0026quot;${obj}\u0026quot; - ‚òï {\u0026quot;foo\u0026quot;:\u0026quot;baa\u0026quot;,\u0026quot;key\u0026quot;:\u0026quot;xxx\u0026quot;} Special variables #  There are special variables injected by lxd-compose automatically:\n   Variable Description     node Contains the JSON object of the node related to the hook.   project Contains the JSON object of the running project.     Diagnose variables #  To help people on parsing JSON object and check generated variable there is a debug tool to print variables inside a project:\n$\u0026gt; # Generate the yaml of all project variables $\u0026gt; lxd-compose diagnose vars lxd-compose-example1 $\u0026gt; # Generate the JSON of all project variables $\u0026gt; lxd-compose diagnose vars lxd-compose-example1 --json "})})()